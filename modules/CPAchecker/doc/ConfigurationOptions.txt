# This is an auto-generated file, DO NOT EDIT!
# Run ant to generate it.

# Possible log levels in descending order 
# (lower levels include higher ones):
# OFF:      no logs published
# SEVERE:   error messages
# WARNING:  warnings
# INFO:     messages
# FINE:     logs on main application level
# FINER:    logs on central CPA algorithm level
# FINEST:   logs published by specific CPAs
# ALL:      debugging information
# Care must be taken with levels of FINER or lower, as output files may
# become quite large and memory usage might become an issue.

# single levels to be excluded from being logged
log.consoleExclude = []

# log level of console output
log.consoleLevel = Level.INFO

# name of the log file
log.file = "CPALog.txt"

# single levels to be excluded from being logged
log.fileExclude = []

# log level of file output
log.level = Level.OFF

# maximum size of log output strings before they will be truncated
log.truncateSize = 10000

# use colors for log messages on console
log.useColors = true

# disable all default output files
# (any explicitly given file will still be written)
output.disable = false

# directory to put all output files in
output.path = "output/"

# base directory for all input & output files
# (except for the configuration file itself)
rootDirectory = "."

# Default rounding mode for floating point operations.
solver.floatingPointRoundingMode = NEAREST_TIES_TO_EVEN
  enum:     [NEAREST_TIES_TO_EVEN, NEAREST_TIES_AWAY, TOWARD_POSITIVE, TOWARD_NEGATIVE,
             TOWARD_ZERO]

# Export solver queries in SmtLib format into a file.
solver.logAllQueries = false
solver.logfile = no default value

# Further options that will be passed to Mathsat in addition to the default
# options. Format is 'key1=value1,key2=value2'
solver.mathsat5.furtherOptions = ""

# Load less stable optimizing version of mathsat5 solver.
solver.mathsat5.loadOptimathsat5 = false

# log all queries as Princess-specific Scala code
solver.princess.logAllQueriesAsScala = false

# file for Princess-specific dump of queries as Scala code
solver.princess.logAllQueriesAsScalaFile = "princess-query-%03d-"

# The number of atoms a term has to have before it gets abbreviated if there
# are more identical terms.
solver.princess.minAtomsForAbbreviation = 100

# Random seed for SMT solver.
solver.randomSeed = 42

# Double check generated results like interpolants and models whether they
# are correct
solver.smtinterpol.checkResults = false

# Further options that will be set to true for SMTInterpol in addition to the
# default options. Format is 'option1,option2,option3'
solver.smtinterpol.furtherOptions = []

# Which SMT solver to use.
solver.solver = SMTINTERPOL
  enum:     [MATHSAT5, SMTINTERPOL, Z3, PRINCESS]

# Log solver actions, this may be slow!
solver.useLogger = false

# Dump failed interpolation queries to this file in SMTLIB2 format
solver.z3.dumpFailedInterpolationQueries = no default value

# Activate replayable logging in Z3. The log can be given as an input to the
# solver and replayed.
solver.z3.log = no default value

# Ordering for objectives in the optimization context
solver.z3.objectivePrioritizationMode = "box"
  allowed values: [lex, pareto, box]

# Engine to use for the optimization
solver.z3.optimizationEngine = "basic"
  allowed values: [basic, farkas, symba]

# Require proofs from SMT solver
solver.z3.requireProofs = true

# Whether to use PhantomReferences for discarding Z3 AST
solver.z3.usePhantomReferences = false

# maximum number of condition adjustments (-1 for infinite)
adjustableconditions.adjustmentLimit = -1

# use a BMC like algorithm that checks for satisfiability after the analysis
# has finished, works only with PredicateCPA
analysis.algorithm.BMC = false

# use CBMC as an external tool from CPAchecker
analysis.algorithm.CBMC = false

# use CEGAR algorithm for lazy counter-example guided analysis
# You need to specify a refiner with the cegar.refiner option.
# Currently all refiner require the use of the ARGCPA.
analysis.algorithm.CEGAR = false

# use a analysis which proves if the program satisfies a specified property
# with the help of an enabler CPA to separate differnt program paths
analysis.algorithm.analysisWithEnabler = false

# use adjustable conditions algorithm
analysis.algorithm.conditionAdjustment = false

# Use McMillan's Impact algorithm for lazy interpolation
analysis.algorithm.impact = false

# use PDR algorithm
analysis.algorithm.pdr = false

# use a proof check algorithm to validate a previously generated proof
analysis.algorithm.proofCheck = false

# do analysis and then check if reached set fulfills property specified by
# ConfigurableProgramAnalysisWithPropertyChecker
analysis.algorithm.propertyCheck = false

# Use termination algorithm to prove (non-)termination.
analysis.algorithm.termination = false

# use a second model checking run (e.g., with CBMC or a different CPAchecker
# configuration) to double-check counter-examples
analysis.checkCounterexamples = false

# use counterexample check and the BDDCPA Restriction option
analysis.checkCounterexamplesWithBDDCPARestriction = false

# do analysis and then check analysis result
analysis.checkProof = false

# use assumption collecting algorithm
analysis.collectAssumptions = false

# continue analysis after a unsupported code was found on one path
analysis.continueAfterUnsupportedCode = false

# stop CPAchecker after startup (internal option, not intended for users)
analysis.disable = false

# entry function
analysis.entryFunction = "main"

# do analysis and then extract pre- and post conditions for custom
# instruction from analysis result
analysis.extractRequirements.customInstruction = false

# create all potential function pointer call edges
analysis.functionPointerCalls = true

# Create edge for skipping a function pointer call if its value is unknown.
analysis.functionPointerEdgesForUnknownPointer = true

# potential targets for call edges created for function pointer calls
analysis.functionPointerTargets = {
          FunctionSet.USED_IN_CODE, FunctionSet.RETURN_VALUE, FunctionSet.EQ_PARAM_TYPES}

# What CFA nodes should be the starting point of the analysis?
analysis.initialStatesFor = Sets.newHashSet(InitialStatesFor.ENTRY)

# run interprocedural analysis
analysis.interprocedural = true

# the machine model, which determines the sizes of types like int
analysis.machineModel = LINUX32
  enum:     [LINUX32, LINUX64]

# Use as targets for call edges only those shich are assigned to the
# particular expression (structure field).
analysis.matchAssignedFunctionPointers = false

# memorize previously used (incomplete) reached sets after a restart of the
# analysis
analysis.memorizeReachedAfterRestart = false

# Partition the initial states based on the type of location they were
# created for (see 'initialStatesFor')
analysis.partitionInitialStates = false

# A String, denoting the programs to be analyzed
analysis.programNames = no default value

# which reached set implementation to use?
# NORMAL: just a simple set
# LOCATIONMAPPED: a different set per location (faster, states with different
# locations cannot be merged)
# PARTITIONED: partitioning depending on CPAs (e.g Location, Callstack etc.)
# PSEUDOPARTITIONED: based on PARTITIONED, uses additional info about the
# states' lattice (maybe faster for some special analyses which use merge_sep
# and stop_sep
analysis.reachedSet = PARTITIONED
  enum:     [NORMAL, LOCATIONMAPPED, PARTITIONED, PSEUDOPARTITIONED]

# restart the analysis using a different configuration after unknown result
analysis.restartAfterUnknown = false

# run a sequence of analysis, where the previous ARG is inserted into the
# current ARGReplayCPA.
analysis.restartAlgorithmWithARGReplay = false

# stop after the first error has been found
analysis.stopAfterError = true

# create summary call statement edges
analysis.summaryEdges = false

# traverse in the order defined by the values of an automaton variable
analysis.traversal.byAutomatonVariable = no default value

# which strategy to adopt for visiting states?
analysis.traversal.order = DFS
  enum:     [DFS, BFS, RAND, RANDOM_PATH]

# handle abstract states with more automaton matches first? (only if
# AutomatonCPA enabled)
analysis.traversal.useAutomatonInformation = false

# handle states with a deeper callstack first
# This needs the CallstackCPA instance to have any effect.
analysis.traversal.useCallstack = false

# handle more abstract states (with less information) first? (only for
# ExplicitCPA)
analysis.traversal.useExplicitInformation = false

# handle states with a deeper loopstack first.
analysis.traversal.useLoopstack = false

# handle abstract states with fewer running threads first? (needs
# ThreadingCPA)
analysis.traversal.useNumberOfThreads = false

# Use an implementation of postorder strategy that allows to select a
# secondary strategy that is used if there are two states with the same
# postorder id. The secondary strategy is selected with
# 'analysis.traversal.order'.
analysis.traversal.usePostorder = false

# handle states with a more shallow loopstack first.
analysis.traversal.useReverseLoopstack = false

# Use an implementation of reverse postorder strategy that allows to select a
# secondary strategy that is used if there are two states with the same
# reverse postorder id. The secondary strategy is selected with
# 'analysis.traversal.order'.
analysis.traversal.useReversePostorder = false

# Do not report unknown if analysis terminated, report true (UNSOUND!).
analysis.unknownAsTrue = false

# stop the analysis with the result unknown if the program does not satisfies
# certain restrictions.
analysis.unknownIfUnrestrictedProgram = false

# add declarations for global variables before entry function
analysis.useGlobalVars = true

# Use analyses parallely. The resulting reachedset is the one of the first
# analysis finishing in time. All other analyses are terminated.
analysis.useParallelAnalyses = false

# Add a threshold to the automaton, after so many branches on a path the
# automaton will be ignored (0 to disable)
assumptions.automatonBranchingThreshold = 0

# write collected assumptions as automaton to file
assumptions.automatonFile = "AssumptionAutomaton.txt"

# If it is enabled, automaton does not add assumption which is considered to
# continue path with corresponding this edge.
assumptions.automatonIgnoreAssumptions = false

# write collected assumptions to file
assumptions.export = true
assumptions.file = "assumptions.txt"

# comma-separated list of files with specifications that should be used 
# in a backwards analysis; used if the analysis starts at the target states!
# (see config/specification/ for examples)
backwardSpecification = []

# Size of the BDD cache in relation to the node table size (set to 0 to use
# fixed BDD cache size).
bdd.javabdd.cacheRatio = 0.1

# Initial size of the BDD cache, use 0 for cacheRatio*initTableSize.
bdd.javabdd.cacheSize = 0

# Initial size of the BDD node table in percentage of available Java heap
# memory (only used if initTableSize is 0).
bdd.javabdd.initTableRatio = 0.001

# Initial size of the BDD node table, use 0 for size based on initTableRatio.
bdd.javabdd.initTableSize = 0

# Which BDD package should be used?
# - java:   JavaBDD (default, no dependencies, many features)
# - sylvan: Sylvan (only 64bit Linux, uses multiple threads)
# - cudd:   CUDD (native library required, reordering not supported)
# - micro:  MicroFactory (maximum number of BDD variables is 1024, slow, but
# less memory-comsumption)
# - buddy:  Buddy (native library required)
# - cal:    CAL (native library required)
# - jdd:    JDD
bdd.package = "JAVA"
  allowed values: [JAVA, SYLVAN, CUDD, MICRO, BUDDY, CAL, JDD]

# Granularity of the Sylvan BDD operations cache (recommended values 4-8).
bdd.sylvan.cacheGranularity = 4

# Log2 size of the BDD cache.
bdd.sylvan.cacheSize = 24

# Log2 size of the BDD node table.
bdd.sylvan.tableSize = 26

# Number of worker threads, 0 for automatic.
bdd.sylvan.threads = 0

# Allow reduction of function entries; calculate abstractions always at
# function entries?
blockreducer.allowReduceFunctionEntries = true

# Allow reduction of function exits; calculate abstractions always at
# function exits?
blockreducer.allowReduceFunctionExits = true

# Allow reduction of loop heads; calculate abstractions always at loop heads?
blockreducer.allowReduceLoopHeads = false

# write the reduced cfa to the specified file.
blockreducer.reducedCfaFile = "ReducedCfa.rsf"

# Do at most n summarizations on a node.
blockreducer.reductionThreshold = 100

# Generate additional invariants by induction and add them to the induction
# hypothesis.
bmc.addInvariantsByInduction = true

# If BMC did not find a bug, check whether the bounding did actually remove
# parts of the state space (this is similar to CBMC's unwinding assertions).
bmc.boundingAssertions = true

# Check reachability of target states after analysis (classical BMC). The
# alternative is to check the reachability as soon as the target states are
# discovered, which is done if cpa.predicate.targetStateSatCheck=true.
bmc.checkTargetStates = true

# try using induction to verify programs with loops
bmc.induction = false

# Export auxiliary invariants used for induction.
bmc.invariantsExport = "invariants.graphml"

# Propagates the interrupts of the invariant generator.
bmc.propagateInvGenInterrupts = false

# File name where to put the path program that is generated as input for
# CBMC. A temporary file is used if this is unspecified. If specified, the
# file name should end with '.i' because otherwise CBMC runs the
# pre-processor on the file.
cbmc.dumpCBMCfile = no default value

# specify the name of the error label
cbmc.options.errorLabel = "ERROR"

# set width of int (16, 32 or 64)
cbmc.options.intWidth = 32

# disable unwinding assertions violation error
cbmc.options.nuaf = false

# specify the limit for unwindings (0 is infinite)
cbmc.options.unwindings = 0

# maximum time limit for CBMC (0 is infinite)
# maximum time limit for CBMC (use milliseconds or specify a unit; 0 for
# infinite)
cbmc.timelimit = 0
cbmc.timelimit = 0ms

# continue analysis after a failed refinement (e.g. due to interpolation)
# other paths may still contain errors that could be found
cegar.continueAfterFailedRefinement = false

# if this score is exceeded by the first analysis, the auxilliary analysis
# will be refined
cegar.domainScoreThreshold = 1024

# Whether to do refinement immediately after finding an error state, or
# globally after the ARG has been unrolled completely.
# whether or not global refinement is performed
cegar.globalRefinement = false

# Which refinement algorithm to use? (give class name, required for CEGAR) If
# the package name starts with 'org.sosy_lab.cpachecker.', this prefix can be
# omitted.
cegar.refiner = no default value

# whether or not to use refinement selection to decide which domain to refine
cegar.useRefinementSelection = false

# Allow then/else branches to be swapped in order to obtain simpler
# conditions.
cfa.allowBranchSwapping = true

# Which functions should be interpreted as encoding assumptions
cfa.assumeFunctions = {"__VERIFIER_assume"}

# dump a simple call graph
cfa.callgraph.export = true

# file name for call graph as .dot file
cfa.callgraph.file = "functionCalls.dot"

# how often do we clone a function?
cfa.cfaCloner.numberOfCopies = 5

# while this option is activated, before each use of a PointerExpression, or
# a dereferenced field access the expression is checked if it is 0
cfa.checkNullPointers = false

# Whether to have a single target node per function for all invalid null
# pointer dereferences or to have separate nodes for each dereference
cfa.checkNullPointers.singleTargetPerFunction = true

# This option enables the computation of a classification of CFA nodes.
cfa.classifyNodes = false

# When a function pointer array element is written with a variable as index,
# create a series of if-else edges with explicit indizes instead.
cfa.expandFunctionPointerArrayAssignments = false

# export CFA as .dot file
cfa.export = true

# export individual CFAs for function as .dot files
cfa.exportPerFunction = true

# export CFA as .dot file
cfa.file = "cfa.dot"

# By enabling this option the variables that are live are computed for each
# edge of the cfa. Live means that their value is read later on.
cfa.findLiveVariables = false

# how often can a function appear in the callstack as a clone of the original
# function?
cfa.functionCalls.recursionDepth = 5

# Also initialize local variables with default values, or leave them
# uninitialized.
cfa.initializeAllVariables = false

# With this option, all declarations in each function will be movedto the
# beginning of each function. Do only use this option if you arenot able to
# handle initializer lists and designated initializers (like they can be used
# for arrays and structs) in your analysis anyway. this option will otherwise
# create c code which is not the same as the original one
cfa.moveDeclarationsToFunctionStart = false

# Show messages when dead code is encountered during parsing.
cfa.showDeadCode = true

# Remove all edges which don't have any effect on the program
cfa.simplifyCfa = true

# simplify simple const expressions like 1+2
cfa.simplifyConstExpressions = true

# simplify pointer expressions like s->f to (*s).f with this option the cfa
# is simplified until at maximum one pointer is allowed for left- and
# rightHandSide
cfa.simplifyPointerExpressions = false

# This option causes the control flow automaton to be transformed into the
# automaton of an equivalent program with one single loop and an artificial
# program counter.
cfa.transformIntoSingleLoop = false

# Single loop transformation builds a decision tree based on the program
# counter values. This option causes the last program counter value not to be
# explicitly assumed in the decision tree, so that it is only indirectly
# represented by the assumption of falsehood for all other assumptions in the
# decision tree.
cfa.transformIntoSingleLoop.omitExplicitLastProgramCounterAssumption = false

# This option controls the size of the subgraphs referred to by program
# counter values. The larger the subgraphs, the fewer program counter values
# are required. Possible values are  MULTIPLE_PATHS, SINGLE_PATH and
# SINGLE_EDGE, where MULTIPLE_PATHS has the largest subgraphs (and fewest
# program counter values) and SINGLE_EDGE has the smallest subgraphs (and
# most program counter values). The larger the subgraphs, the closer the
# resulting graph will look like the original CFA.
cfa.transformIntoSingleLoop.subgraphGrowthStrategy = MULTIPLE_PATHS
  enum:     [MULTIPLE_PATHS, SINGLE_PATH, SINGLE_EDGE]

# clone functions of the CFA, such that there are several identical CFAs for
# each function, only with different names.
cfa.useCFACloningForMultiThreadedPrograms = false

# unwind recursive functioncalls (bounded to max call stack size)
cfa.useFunctionCallUnwinding = false

# Dump domain type statistics to a CSV file.
cfa.variableClassification.domainTypeStatisticsFile = no default value

# Dump variable classification to a file.
cfa.variableClassification.logfile = "VariableClassification.log"

# Print some information about the variable classification.
cfa.variableClassification.printStatsOnStartup = false

# Dump variable type mapping to a file.
cfa.variableClassification.typeMapFile = "VariableTypeMapping.txt"

# Output an input file, with invariants embedded as assume constraints.
cinvariants.export = false

# Prefix to add to an output file, which would contain assumed invariants.
cinvariants.prefix = no default value

# Attempt to simplify the invariant before exporting [may be very expensive].
cinvariants.simplify = false

# Dump the complete configuration to a file.
configuration.dumpFile = "UsedConfiguration.properties"

# Which model checker to use for verifying counterexamples as a second check.
# Currently CBMC or CPAchecker with a different config or the concrete
# execution 
# checker can be used.
counterexample.checker = CBMC
  enum:     [CBMC, CPACHECKER, CONCRETE_EXECUTION]

# configuration file for counterexample checks with CPAchecker
counterexample.checker.config = no default value

# File name where to put the path specification that is generated as input
# for the counterexample check. A temporary file is used if this is
# unspecified.
counterexample.checker.path.file = no default value

# The file in which the generated C code is saved.
counterexample.concrete.dumpFile = no default value

# Path to the compiler. Can be absolute or only the name of the program if it
# is in the PATH
counterexample.concrete.pathToCompiler = "/usr/bin/gcc"

# Maximum time limit for the concrete execution checker.
# This limit is used for compilation as well as execution so overall, twice
# the time of this limit may be consumed.
# (use milliseconds or specify a unit; 0 for infinite)
counterexample.concrete.timelimit = 0ms

# continue analysis after an counterexample was found that was denied by the
# second check
counterexample.continueAfterInfeasibleError = true

# If the option assumeLinearArithmetics is set, this option can be used to
# allow division and modulo by constants.
counterexample.export.assumptions.allowDivisionAndModuloByConstants = false

# If the option assumeLinearArithmetics is set, this option can be used to
# allow multiplication between operands with at least one constant.
counterexample.export.assumptions.allowMultiplicationWithConstants = false

# Try to avoid using operations that exceed the capabilities of linear
# arithmetics when extracting assumptions from the model. This option aims to
# prevent witnesses that are inconsistent with  models that are, due to an
# analysis limited to linear arithmetics, actually incorrect.
#  This option does not magically produce a correct witness from an incorrect
# model, and since the difference between an incorrect witness consistent
# with the model and an incorrect witness that is inconsistent with the model
# is academic, you usually want this option to be off.
counterexample.export.assumptions.assumeLinearArithmetics = false

# Whether or not to include concrete address values.
counterexample.export.assumptions.includeConstantsForPointers = true

# export counterexample as automaton
counterexample.export.automaton = "Counterexample.%d.spc"

# exports either CMBC format or a concrete path program
counterexample.export.codeStyle = CBMC
  enum:     [CBMC, CONCRETE_EXECUTION]

# compress the produced error-witness automata using GZIP compression.
counterexample.export.compressErrorWitness = true

# export counterexample core as text file
counterexample.export.core = "Counterexample.%d.core.txt"

# export counterexample to file, if one is found
counterexample.export.enabled = true

# export counterexample as source file
counterexample.export.exportAsSource = true

# export test harness
counterexample.export.exportHarness = false

# export error paths to files immediately after they were found
counterexample.export.exportImmediately = false

# export counterexample as witness/graphml file
counterexample.export.exportWitness = true

# export counterexample as text file
counterexample.export.file = "Counterexample.%d.txt"

# Filter for irrelevant counterexamples to reduce the number of similar
# counterexamples reported. Only relevant with analysis.stopAfterError=false
# and counterexample.export.exportImmediately=true. Put the weakest and
# cheapest filter first, e.g., PathEqualityCounterexampleFilter.
counterexample.export.filters = no default value

# where to dump the counterexample formula in case a specification violation
# is found
counterexample.export.formula = "Counterexample.%d.smt2"

# export counterexample as graph
counterexample.export.graph = "Counterexample.%d.dot"

# export counterexample to file as GraphML automaton
counterexample.export.graphml = "Counterexample.%d.graphml"

# export test harness to file as code
counterexample.export.harness = "Counterexample.%d.harness.c"

# where to dump the counterexample model in case a specification violation is
# found
counterexample.export.model = "Counterexample.%d.assignment.txt"

# The files where the BDDCPARestrictionAlgorithm should write the presence
# conditions for the counterexamples to.
counterexample.export.presenceCondition = "Counterexample.%d.presenceCondition.txt"

# File name for analysis report in case a counterexample was found.
counterexample.export.report = "Counterexample.%d.html"

# export counterexample as source file
counterexample.export.source = "Counterexample.%d.c"

# If continueAfterInfeasibleError is true, remove the infeasible
# counterexample before continuing.Setting this to false may prevent a lot of
# similar infeasible counterexamples to get discovered, but is unsound
counterexample.removeInfeasibleErrors = false

# Compute and export information about the verification coverage?
coverage.enabled = true

# print coverage info to file
coverage.export = true
coverage.file = "coverage.info"

# How should coverage be determined? REACHED: from the final ARG, TRANSFER:
# from the edges that explored by the transfer relation
coverage.mode = NONE
  enum:     [NONE, REACHED, TRANSFER]

# print coverage summary to stdout
coverage.stdout = true

# CPA to use (see doc/Configuration.txt for more documentation on this)
cpa = CompositeCPA.class.getCanonicalName()

# Where to perform abstraction
cpa.abe.abstractionLocations = LOOPHEAD
  enum:     [ALL, LOOPHEAD, MERGE]

# Check target states reachability
cpa.abe.checkTargetStates = true

# Cache formulas produced by path formula manager
cpa.abe.useCachingPathFormulaManager = true

# Use this to change the underlying abstract domain in the APRON library
cpa.apron.domain = OCTAGON
  enum:     [BOX, OCTAGON, POLKA, POLKA_STRICT, POLKA_EQ]

# get an initial precision from file
cpa.apron.initialPrecisionFile = no default value

# this option determines which initial precision should be used
cpa.apron.initialPrecisionType = "STATIC_FULL"
  allowed values: [STATIC_FULL, REFINEABLE_EMPTY]

# with this option enabled the states are only merged at loop heads
cpa.apron.mergeop.onlyMergeAtLoopHeads = false

# of which type should the merge be?
cpa.apron.mergeop.type = "SEP"
  allowed values: [SEP, JOIN, WIDENING]

# target file to hold the exported precision
cpa.apron.precisionFile = no default value

# whether or not to check for repeated refinements, to then reset the
# refinement root
cpa.apron.refiner.checkForRepeatedRefinements = true

# Timelimit for the backup feasibility check with the apron analysis.(use
# seconds or specify a unit; 0 for infinite)
cpa.apron.refiner.timeForApronFeasibilityCheck = 0ns

# split disequalities considering integer operands into two states or use
# disequality provided by apron library 
cpa.apron.splitDisequalities = true

# inform merge operator in CPA enabled analysis that it should delete the
# subgraph of the merged node which is required to get at most one successor
# per CFA edge.
cpa.arg.deleteInCPAEnabledAnalysis = false

# Dump all ARG related statistics files after each iteration of the CPA
# algorithm? (for debugging and demonstration)
cpa.arg.dumpAfterIteration = false

# export final ARG as .dot file
cpa.arg.export = true
cpa.arg.file = "ARG.dot"

# inform ARG CPA if it is run in an analysis with enabler CPA because then it
# must behave differently during merge.
# inform ARG CPA if it is run in a CPA enabled analysis because then it
# mustbehave differntly during merge.
cpa.arg.inCPAEnabledAnalysis = false

# whether to keep covered states in the reached set as addition to keeping
# them in the ARG
cpa.arg.keepCoveredStatesInReached = false

# export a proof as .graphml file
cpa.arg.proofWitness = no default value

# export simplified ARG that shows all refinements to .dot file
cpa.arg.refinements.file = "ARGRefinements.dot"

# export final ARG as .dot file, showing only loop heads and function
# entries/exits
cpa.arg.simplifiedARG.file = "ARGSimplified.dot"

# Verification witness: Include the considered case of an assume?
cpa.arg.witness.exportAssumeCaseInfo = true

# Verification witness: Include assumptions (C statements)?
cpa.arg.witness.exportAssumptions = true

# Verification witness: Include function calls and function returns?
cpa.arg.witness.exportFunctionCallsAndReturns = true

# Verification witness: Include the (starting) line numbers of the operations
# on the transitions?
cpa.arg.witness.exportLineNumbers = true

# Verification witness: Include the offset within the file?
cpa.arg.witness.exportOffset = true

# Verification witness: Include the sourcecode of the operations?
cpa.arg.witness.exportSourcecode = true

# Verification witness: Include an thread-identifier within the file?
cpa.arg.witness.exportThreadId = false

# Verification witness: Revert escaping/renaming of functions for threads?
cpa.arg.witness.revertThreadFunctionRenaming = false

# signal the analysis to break in case the given number of error state is
# reached 
cpa.automaton.breakOnTargetState = 1

# export automaton to file
cpa.automaton.dotExport = false

# file for saving the automaton in DOT format (%s will be replaced with
# automaton name)
cpa.automaton.dotExportFile = "%s.dot"

# the maximum number of iterations performed after the initial error is
# found, despite the limitgiven as cpa.automaton.breakOnTargetState is not
# yet reached
cpa.automaton.extraIterationsLimit = -1

# file with automaton specification for ObserverAutomatonCPA and
# ControlAutomatonCPA
cpa.automaton.inputFile = no default value

# Merge two automata states if one of them is TOP.
cpa.automaton.mergeOnTop = false

# An implicit precision: consider states with a self-loop and no other
# outgoing edges as TOP.
cpa.automaton.prec.topOnFinalSelfLoopingState = false

# Whether to treat automaton states with an internal error state as targets.
# This should be the standard use case.
cpa.automaton.treatErrorsAsTargets = true

# If enabled, cache queries also consider blocks with non-matching precision
# for reuse.
cpa.bam.aggressiveCaching = true

# export blocked ARG as .dot file
cpa.bam.argFile = "BlockedARG.dot"

# Type of partitioning (FunctionAndLoopPartitioning or
# DelayedFunctionAndLoopPartitioning)
# or any class that implements a PartitioningHeuristic
cpa.bam.blockHeuristic = no default value

# only consider function with a minimum number of calls. This approach is
# similar to 'inlining' functions used only a few times. Info: If a function
# is called several times in a loop, we only count 'one' call.
cpa.bam.blockHeuristic.functionPartitioning.minFunctionCalls = 0

# only consider function with a minimum number of CFA nodes. This approach is
# similar to 'inlining' small functions, when using BAM.
cpa.bam.blockHeuristic.functionPartitioning.minFunctionSize = 0

# This flag determines which precisions should be updated during refinement.
# We can choose between the minimum number of states and all states that are
# necessary to re-explore the program along the error-path.
cpa.bam.doPrecisionRefinementForAllStates = false

# export blocks
cpa.bam.exportBlocksPath = "block_cfa.dot"

# If enabled, the reached set cache is analysed for each cache miss to find
# the cause of the miss.
cpa.bam.gatherCacheMissStatistics = false

# BAM allows to analyse recursive procedures. This strongly depends on the
# underlying CPA. The current support includes only ValueAnalysis and
# PredicateAnalysis (with tree interpolation enabled).
cpa.bam.handleRecursiveProcedures = false

# export single blocked ARG as .dot files, should contain '%d'
cpa.bam.indexedArgFile = "ARGs/ARG_%d.dot"

# if we cannot determine a repeating/covering call-state, we will run into
# CallStackOverflowException. Thus we bound the stack size (unsound!). This
# option only limits non-covered recursion, but not a recursion where we find
# a coverage and re-use the cached block several times. The value '-1'
# disables this option.
cpa.bam.maximalDepthForExplicitRecursion = -1

# export used parts of blocked ARG as .dot file
cpa.bam.simplifiedArgFile = "BlockedARGSimplified.dot"

# Use more fast partitioning builder, which can not handle loops
cpa.bam.useExtendedPartitioningBuilder = false

# max bitsize for values and vars, initial value
cpa.bdd.bitsize = 64

# use a smaller bitsize for all vars, that have only intEqual values
cpa.bdd.compressIntEqual = true

# declare the bits of a var from 0 to N or from N to 0
cpa.bdd.initBitsIncreasing = true

# declare first bit of all vars, then second bit,...
cpa.bdd.initBitwise = true

# declare vars partitionwise
cpa.bdd.initPartitions = Ordered = true

# declare partitions ordered
cpa.bdd.initPartitionsOrdered = true

# Dump tracked variables to a file.
cpa.bdd.logfile = "BDDCPA_tracked_variables.log"

# mergeType
cpa.bdd.merge = "join"

# Threshold for unrolling blocks before stopping. A value of 0 disables the
# threshold.
cpa.blockcount.threshold = 1

# Number of loop iterations before the loop counter is abstracted. Zero is
# equivalent to no limit.
cpa.bounds.loopIterationsBeforeAbstraction = 0

# this option controls how the maxLoopIterations condition is adjusted when a
# condition adjustment is invoked.
cpa.bounds.maxLoopIterationAdjusterFactory = STATIC
  enum:     [STATIC, INCREMENT, DOUBLE]

# threshold for unrolling loops of the program (0 is infinite)
# works only if assumption storage CPA is enabled, because otherwise it would
# be unsound
cpa.bounds.maxLoopIterations = 0

# threshold for adjusting the threshold for unrolling loops of the program (0
# is infinite).
# only relevant in combination with a non-static maximum loop iteration
# adjuster.
cpa.bounds.maxLoopIterationsUpperBound = 0

# threshold for unwinding recursion (0 is infinite)
# works only if assumption storage CPA is enabled, because otherwise it would
# be unsound
cpa.bounds.maxRecursionDepth = 0

# depth of recursion bound
cpa.callstack.depth = 0

# which abstract domain to use for callstack cpa, typically FLAT which is
# faster since it uses only object equivalence
cpa.callstack.domain = "FLAT"
  allowed values: [FLAT, FLATPCC]

# Skip recursion if it happens only by going via a function pointer (this is
# unsound). Imprecise function pointer tracking often lead to false
# recursions.
cpa.callstack.skipFunctionPointerRecursion = false

# Skip recursion (this is unsound). Treat function call as a statement (the
# same as for functions without bodies)
cpa.callstack.skipRecursion = false

# Skip recursion if it happens only by going via a void function (this is
# unsound).
cpa.callstack.skipVoidRecursion = false

# analyse the CFA backwards
cpa.callstack.traverseBackwards = false

# unsupported functions cause an exception
cpa.callstack.unsupportedFunctions = {"pthread_create"}

# firing relation to be used in the precision adjustment operator
cpa.chc.firingRelation = "Always"
  allowed values: [Always, Maxcoeff, Sumcoeff, Homeocoeff]

# generalization operator to be used in the precision adjustment operator
cpa.chc.generalizationOperator = "Widen"
  allowed values: [Top, Widen, WidenMax, WidenSum]

# merge operator to be used
cpa.chc.merge = "SEP"
  allowed values: [SEP, JOIN]

# By enabling this option the CompositeTransferRelation will compute abstract
# successors for as many edges as possible in one call. For any chain of
# edges in the CFA which does not have more than one outgoing or leaving edge
# the components of the CompositeCPA are called for each of the edges in this
# chain. Strengthening is still computed after every edge. The main
# difference is that while this option is enabled not every ARGState may have
# a single edge connecting to the child/parent ARGState but it may instead be
# a list.
cpa.composite.aggregateBasicBlocks = false

# inform Composite CPA if it is run in a CPA enabled analysis because then it
# must behave differently during merge.
cpa.composite.inCPAEnabledAnalysis = false

# which composite merge operator to use (plain or agree)
# Both delegate to the component cpas, but agree only allows merging if all
# cpas agree on this. This is probably what you want.
cpa.composite.merge = "AGREE"
  allowed values: [PLAIN, AGREE]

# Limit for Java heap memory used by CPAchecker (in MB, not MiB!; -1 for
# infinite)
cpa.conditions.global.memory.heap = -1

# Limit for process memory used by CPAchecker (in MB, not MiB!; -1 for
# infinite)
cpa.conditions.global.memory.process = -1

# Limit for size of reached set (-1 for infinite)
cpa.conditions.global.reached.size = -1

# Limit for cpu time used by CPAchecker (use milliseconds or specify a unit;
# -1 for infinite)
cpa.conditions.global.time.cpu = -1

# Hard limit for cpu time used by CPAchecker (use milliseconds or specify a
# unit; -1 for infinite)
# When using adjustable conditions, analysis will end after this threshold
cpa.conditions.global.time.cpu.hardlimit = -1

# Limit for wall time used by CPAchecker (use milliseconds or specify a unit;
# -1 for infinite)
cpa.conditions.global.time.wall = -1

# Hard limit for wall time used by CPAchecker (use milliseconds or specify a
# unit; -1 for infinite)
# When using adjustable conditions, analysis will end after this threshold
cpa.conditions.global.time.wall.hardlimit = -1

# determines if there should be one single assignment state per state, one
# per path segment between assume edges, or a global one for the whole
# program.
cpa.conditions.path.assignments.scope = STATE
  enum:     [STATE, PATH, PROGRAM]

# sets the threshold for assignments (-1 for infinite), and it is upto, e.g.,
# ValueAnalysisPrecisionAdjustment to act accordingly to this threshold
# value.
cpa.conditions.path.assignments.threshold = DISABLED

# maximum number of assume edges length (-1 for infinite)
cpa.conditions.path.assumeedges.limit = -1

# The condition
cpa.conditions.path.condition = no default value

# maximum path length (-1 for infinite)
cpa.conditions.path.length.limit = -1

# maximum repetitions of any edge in a path (-1 for infinite)
cpa.conditions.path.repetitions.limit = -1

# Generate congruences for sums of variables (<=> x and y have same/different
# evenness)
cpa.congruence.trackCongruenceSum = false

# Cache formulas produced by path formula manager
cpa.congruence.useCachingPathFormulaManager = true

# Type of less-or-equal operator to use
cpa.constraints.lessOrEqualType = SUBSET
  enum:     [SUBSET, ALIASED_SUBSET, IMPLICATION]

# Type of merge operator to use
cpa.constraints.mergeType = SEP
  enum:     [SEP, JOIN_FITTING_CONSTRAINT]

# Type of precision to use. Has to be LOCATION if PredicateExtractionRefiner
# is used.
cpa.constraints.refinement.precisionType = CONSTRAINTS
  enum:     [CONSTRAINTS, LOCATION]

# Whether to remove constraints that can't add any more information
# toanalysis during simplification
cpa.constraints.removeOutdated = true

# Whether to remove trivial constraints from constraints states during
# simplification
cpa.constraints.removeTrivial = false

# When to check the satisfiability of constraints
cpa.constraints.satCheckStrategy = AT_ASSUME
  enum:     [AT_ASSUME, AT_TARGET]

# which merge operator to use for DefUseCPA
cpa.defuse.merge = "sep"
  allowed values: [sep, join]

# Which strategy to use for forced coverings (empty for none)
cpa.forcedCovering = no default value

# When an invalid function pointer is called, do not assume all functions as
# possible targets and instead call no function.
cpa.functionpointer.ignoreInvalidFunctionPointerCalls = false

# When an unknown function pointer is called, do not assume all functions as
# possible targets and instead call no function (this is unsound).
cpa.functionpointer.ignoreUnknownFunctionPointerCalls = false

# whether function pointers with invalid targets (e.g., 0) should be tracked
# in order to find calls to such pointers
cpa.functionpointer.trackInvalidFunctionPointers = false

# which default SecurityClass to use for PolicyEnforcementCPA
cpa.ifcsecurity.defaultsc = "LOW"

# which immediatechecksfile to use for PolicyEnforcementCPA
cpa.ifcsecurity.immediatechecksfile = "immediatechecks.conf"

# defines security classes of entities
cpa.ifcsecurity.initialmapfile = no default value

# which merge operator to use for ControlDependencyTrackerCPA
# which merge operator to use for DependencyTrackerCPA
# which merge operator to use for PolicyEnforcementCPA
cpa.ifcsecurity.merge = "JOIN"
  allowed values: [SEP, JOIN]

# which policy to use for PolicyEnforcementCPA
cpa.ifcsecurity.policy = "HILO"

# which betamapfile to use for PolicyEnforcementCPA
cpa.ifcsecurity.scMappingFile = "betamap.conf"

# which states shall be checked
cpa.ifcsecurity.statestocheck = 0

# which stop operator to use for ControlDependencyTrackerCPA
# which stop operator to use for DependencyTrackerCPA
# which stop operator to use for PolicyEnforcementCPA
cpa.ifcsecurity.stop = "SEP"
  allowed values: [SEP, JOIN]

# which type of merge operator to use for IntervalAnalysisCPA
cpa.interval.merge = "SEP"
  allowed values: [SEP, JOIN]

# decides whether one (false) or two (true) successors should be created when
# an inequality-check is encountered
cpa.interval.splitIntervals = false

# at most that many intervals will be tracked per variable, -1 if number not
# restricted
cpa.interval.threshold = -1

# controls whether to use abstract evaluation always, never, or depending on
# entering edges.
cpa.invariants.abstractionStateFactory = ENTERING_EDGES
  enum:     [ALWAYS, ENTERING_EDGES, NEVER]

# determine variables relevant to the decision whether or not a target path
# assume edge is taken and limit the analyis to those variables.
cpa.invariants.analyzeRelevantVariablesOnly = true

# determine target locations in advance and analyse paths to the target
# locations only.
cpa.invariants.analyzeTargetPathsOnly = true

# controls the condition adjustment logic: STATIC means that condition
# adjustment is a no-op, INTERESTING_VARIABLES increases the interesting
# variable limit, MAXIMUM_FORMULA_DEPTH increases the maximum formula depth,
# ABSTRACTION_STRATEGY tries to choose a more precise abstraction strategy
# and COMPOUND combines the other strategies (minus STATIC).
cpa.invariants.conditionAdjusterFactory = COMPOUND
  enum:     [STATIC, INTERESTING_VARIABLES, MAXIMUM_FORMULA_DEPTH,
             ABSTRACTION_STRATEGY, COMPOUND]

# include type information for variables, such as x >= MIN_INT && x <=
# MAX_INT
cpa.invariants.includeTypeInformation = true

# the maximum number of variables to consider as interesting. -1 one disables
# the limit, but this is not recommended. 0 means that guessing interesting
# variables is disabled.
cpa.invariants.interestingVariableLimit = 2

# the maximum number of adjustments of the interestingVariableLimit. -1 one
# disables the limit
cpa.invariants.maxInterestingVariableAdjustments = -1

# the maximum tree depth of a formula recorded in the environment.
cpa.invariants.maximumFormulaDepth = 4

# which merge operator to use for InvariantCPA
cpa.invariants.merge = "PRECISIONDEPENDENT"
  allowed values: [JOIN, SEP, PRECISIONDEPENDENT]

# With this option the handling of global variables during the analysis can
# be fine-tuned. For example while doing a function-wise analysis it is
# important to assume that all global variables are live. In contrast to
# that, while doing a global analysis, we do not need to assume global
# variables being live.
cpa.liveVar.assumeGlobalVariablesAreAlwaysLive = true

# With this option enabled, unction calls that occur in the CFA are followed.
# By disabling this option one can traverse a function without following
# function calls (in this case FunctionSummaryEdges are used)
cpa.location.followFunctionCalls = true

# this option determines which merge operator to use
cpa.loopinvariants.mergeType = "SEP"
  allowed values: [SEP, MERGE]

# Number of loop iterations before the loop counter is abstracted. Zero is
# equivalent to no limit.
cpa.loopstack.loopIterationsBeforeAbstraction = 0

# this option controls how the maxLoopIterations condition is adjusted when a
# condition adjustment is invoked.
cpa.loopstack.maxLoopIterationAdjusterFactory = STATIC
  enum:     [STATIC, INCREMENT, DOUBLE]

# threshold for unrolling loops of the program (0 is infinite)
# The option is ignored unless AssumptionStorageCPA is enabled (as otherwise
# it is unsound).
cpa.loopstack.maxLoopIterations = 0

# threshold for adjusting the threshold for unrolling loops of the program (0
# is infinite).
# only relevant in combination with a non-static maximum loop iteration
# adjuster.
cpa.loopstack.maxLoopIterationsUpperBound = 0

# Where to perform abstraction
cpa.lpi.abstractionLocations = LOOPHEAD
  enum:     [ALL, LOOPHEAD, MERGE]

# Attach extra invariant from other CPAs during the value determination
# computation
cpa.lpi.attachExtraInvariantDuringValueDetermination = true

# Check whether the policy depends on the initial value
cpa.lpi.checkPolicyInitialCondition = true

# Check target states reachability
cpa.lpi.checkTargetStates = true

# Compute abstraction for larger templates using decomposition
cpa.lpi.computeAbstractionByDecomposition = false

# Do not compute the abstraction until strengthen is called. This speeds up
# the computation, but does not let other CPAs use the output of LPI.
cpa.lpi.delayAbstractionUntilStrengthen = false

# Value to substitute for the epsilon
cpa.lpi.epsilon = Rational.ONE

# Generate new templates using polyhedra convex hull
cpa.lpi.generateTemplatesUsingConvexHull = false

# Remove UFs and ITEs from policies.
cpa.lpi.linearizePolicy = true

# Run naive value determination first, switch to namespaced if it fails.
cpa.lpi.runHopefulValueDetermination = true

# Remove redundant items when abstract values.
cpa.lpi.simplifyDotOutput = false

# Algorithm for converting a formula to a set of lemmas
cpa.lpi.toLemmasAlgorithm = "RCNF"
  allowed values: [CNF, RCNF, NONE]

# Number of refinements after which the unrolling depth is increased.Set to
# -1 to never increase the depth.
cpa.lpi.unrollingRefinementThreshold = 2

# Use caching optimization solver
cpa.lpi.useCachingOptSolver = false

# Cache formulas produced by path formula manager
cpa.lpi.useCachingPathFormulaManager = true

# Use the new SSA after the merge operation.
cpa.lpi.useNewSSAAfterMerge = false

# Syntactically pre-compute dependencies for value determination
cpa.lpi.valDetSyntacticCheck = true

# Number of value determination steps allowed before widening is run. Value
# of '-1' runs value determination until convergence.
cpa.lpi.wideningThreshold = -1

# time limit for a single post computation (use milliseconds or specify a
# unit; 0 for infinite)
cpa.monitor.limit = 0

# time limit for all computations on a path in milliseconds (use milliseconds
# or specify a unit; 0 for infinite)
cpa.monitor.pathcomputationlimit = 0

# this option determines which initial precision should be used
cpa.octagon.initialPrecisionType = "STATIC_FULL"
  allowed values: [STATIC_FULL, REFINEABLE_EMPTY]

# with this option enabled the states are only merged at loop heads
cpa.octagon.mergeop.onlyMergeAtLoopHeads = false

# of which type should the merge be?
cpa.octagon.mergeop.type = "SEP"
  allowed values: [SEP, JOIN, WIDENING]

# with this option the number representation in the library will be changed
# between floats and ints.
cpa.octagon.octagonLibrary = "INT"
  allowed values: [INT, FLOAT]

# whether or not to check for repeated refinements, to then reset the
# refinement root
cpa.octagon.refiner.checkForRepeatedRefinements = true

# Timelimit for the backup feasibility check with the octagon analysis.(use
# seconds or specify a unit; 0 for infinite)
cpa.octagon.refiner.timeForOctagonFeasibilityCheck = 0ns

# which merge operator to use for PointerCPA
cpa.pointer2.merge = "JOIN"
  allowed values: [JOIN, SEP]

# which merge operator to use for PointerACPA
cpa.pointerA.merge = "JOIN"
  allowed values: [SEP, JOIN]

# which stop operator to use for PointerACPA
cpa.pointerA.stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

# Predicate ordering
cpa.predicate.abs.predicateOrdering.method = CHRONOLOGICAL
  enum:     [SIMILARITY, FREQUENCY, IMPLICATION, REV_IMPLICATION, RANDOMLY,
             FRAMEWORK_RANDOM, FRAMEWORK_SIFT, FRAMEWORK_SIFTITE, FRAMEWORK_WIN2,
             FRAMEWORK_WIN2ITE, FRAMEWORK_WIN3, FRAMEWORK_WIN3ITE, CHRONOLOGICAL]

# Use multiple partitions for predicates
cpa.predicate.abs.predicateOrdering.partitions = false

# use caching of abstractions
# use caching of region to formula conversions
cpa.predicate.abs.useCache = true

# DEPRECATED: whether to use Boolean (false) or Cartesian (true) abstraction
cpa.predicate.abstraction.cartesian = false

# whether to use Boolean or Cartesian abstraction or both
cpa.predicate.abstraction.computation = BOOLEAN
  enum:     [CARTESIAN, BOOLEAN, COMBINED, ELIMINATION]

# dump the abstraction formulas if they took to long
cpa.predicate.abstraction.dumpHardQueries = false

# Identify those predicates where the result is trivially known before
# abstraction computation and omit them.
cpa.predicate.abstraction.identifyTrivialPredicates = false

# get an initial map of predicates from a list of files (see source
# doc/examples/predmap.txt for an example)
cpa.predicate.abstraction.initialPredicates = []

# Apply location-specific predicates to all locations in their function
cpa.predicate.abstraction.initialPredicates.applyFunctionWide = false

# Apply location- and function-specific predicates globally (to all locations
# in the program)
cpa.predicate.abstraction.initialPredicates.applyGlobally = false

# when reading predicates from file, convert them from Integer- to BV-theory
# or reverse.
cpa.predicate.abstraction.initialPredicates.encodePredicates = DISABLE
  enum:     [DISABLE, INT2BV, BV2INT]

# An initial set of comptued abstractions that might be reusable
cpa.predicate.abstraction.reuseAbstractionsFrom = no default value

# Simplify the abstraction formula that is stored to represent the state
# space. Helpful when debugging (formulas get smaller).
cpa.predicate.abstraction.simplify = false

# What to use for storing abstractions
cpa.predicate.abstraction.type = "BDD"
  allowed values: [BDD, FORMULA]

# Export one abstraction formula for each abstraction state into a file?
cpa.predicate.abstractions.export = true

# file that consists of one abstraction formula for each abstraction state
cpa.predicate.abstractions.file = "abstractions.txt"

# Add constraints for the range of the return-value of a nondet-method. For
# example the assignment 'X=nondet_int()' produces the constraint
# 'MIN<=X<=MAX', where MIN and MAX are computed from the type of the method
# (signature, not name!).
cpa.predicate.addRangeConstraintsForNondet = false

# which strategy/heuristic should be used to compute relevant predicates for
# a block-reduction?
# AUXILIARY: dependencies between variables.
# OCCURENCE: occurence of variables in the block.
# ALL: all variables are relevant.
cpa.predicate.bam.predicateComputer = "AUXILIARY"
  allowed values: [AUXILIARY, OCCURRENCE, ALL]

# force abstractions immediately after threshold is reached (no effect if
# threshold = 0)
cpa.predicate.blk.alwaysAfterThreshold = true

# abstraction always and only on explicitly computed abstraction nodes.
cpa.predicate.blk.alwaysAndOnlyAtExplicitNodes = false

# force abstractions at each branch node, regardless of threshold
cpa.predicate.blk.alwaysAtBranch = false

# force abstractions at the head of the analysis-entry function (first node
# in the body), regardless of threshold
cpa.predicate.blk.alwaysAtEntryFunctionHead = false

# abstraction always at explicitly computed abstraction nodes.
cpa.predicate.blk.alwaysAtExplicitNodes = false

# force abstractions at each function call (node before entering the body),
# regardless of threshold
cpa.predicate.blk.alwaysAtFunctionCallNodes = false

# force abstractions at each function head (first node in the body),
# regardless of threshold
cpa.predicate.blk.alwaysAtFunctionHeads = false

# force abstractions at each function calls/returns, regardless of threshold
cpa.predicate.blk.alwaysAtFunctions = true

# force abstractions at each join node, regardless of threshold
cpa.predicate.blk.alwaysAtJoin = false

# force abstractions at loop heads, regardless of threshold
cpa.predicate.blk.alwaysAtLoops = true

# abstractions at function calls/returns if threshold has been reached (no
# effect if threshold = 0)
cpa.predicate.blk.functions = false

# abstractions at CFA nodes with more than one incoming edge if threshold has
# been reached (no effect if threshold = 0)
cpa.predicate.blk.join = false

# abstractions at loop heads if threshold has been reached (no effect if
# threshold = 0)
cpa.predicate.blk.loops = false

# maximum blocksize before abstraction is forced
# (non-negative number, special values: 0 = don't check threshold, 1 = SBE)
cpa.predicate.blk.threshold = 0

# use caching of path formulas
cpa.predicate.blk.useCache = true

# always check satisfiability at end of block, even if precision is empty
cpa.predicate.checkBlockFeasibility = false

# The default size in bytes for memory allocations when the value cannot be
# determined.
cpa.predicate.defaultAllocationSize = 4

# The default length for arrays when the real length cannot be determined.
cpa.predicate.defaultArrayLength = 20

# Use deferred allocation heuristic that tracks void * variables until the
# actual type of the allocation is figured out.
cpa.predicate.deferUntypedAllocations = true

# Direction of the analysis?
cpa.predicate.direction = FORWARD
  enum:     [FORWARD, BACKWARD]

# Enable the possibility to precompute explicit abstraction locations.
cpa.predicate.enableBlockreducer = false

# Theory to use as backend for bitvectors. If different from BITVECTOR, the
# specified theory is used to approximate bitvectors. This can be used for
# solvers that do not support bitvectors, or for increased performance.
cpa.predicate.encodeBitvectorAs = INTEGER
  enum:     [INTEGER, RATIONAL, BITVECTOR, FLOAT]

# Theory to use as backend for floats. If different from FLOAT, the specified
# theory is used to approximate floats. This can be used for solvers that do
# not support floating-point arithmetic, or for increased performance.
cpa.predicate.encodeFloatAs = RATIONAL
  enum:     [INTEGER, RATIONAL, BITVECTOR, FLOAT]

# Replace possible overflows with an ITE-structure, which returns either the
# normal value or an UF representing the overflow.
cpa.predicate.encodeOverflowsWithUFs = false

# Name of an external function that will be interpreted as if the function
# call would be replaced by an externally defined expression over the program
# variables. This will only work when all variables referenced by the dimacs
# file are global and declared before this function is called.
cpa.predicate.externModelFunctionName = "__VERIFIER_externModelSatisfied"

# where to dump interpolation and abstraction problems (format string)
cpa.predicate.formulaDumpFilePattern = "%s%04d-%s%03d.smt2"

# Handle arrays using the theory of arrays.
cpa.predicate.handleArrays = false

# Handle field access via extract and concat instead of new variables.
cpa.predicate.handleFieldAccess = false

# If disabled, all implicitly initialized fields and elements are treated as
# non-dets
cpa.predicate.handleImplicitInitialization = true

# Handle aliasing of pointers. This adds disjunctions to the formulas, so be
# careful when using cartesian abstraction.
cpa.predicate.handlePointerAliasing = true

# When a string literal initializer is encountered, initialize the contents
# of the char array with the contents of the string literal instead of just
# assigning a fresh non-det address to it
cpa.predicate.handleStringLiteralInitializers = false

# Allows to ignore Concat and Extract Calls when Bitvector theory was
# replaced with Integer or Rational.
cpa.predicate.ignoreExtractConcat = true

# Ignore fields that are not relevant for reachability properties. This is
# unsound in case fields are accessed by pointer arithmetic with hard-coded
# field offsets. Only relvant if ignoreIrrelevantVariables is enabled.
cpa.predicate.ignoreIrrelevantFields = true

# Ignore variables that are not relevant for reachability properties.
cpa.predicate.ignoreIrrelevantVariables = true

# do not include assumptions of states into path formula during strengthening
cpa.predicate.ignoreStateAssumptions = false

# Add computed invariants to the precision. Invariants do not need to be
# generated with the PredicateCPA they can also be given from outside.
cpa.predicate.invariants.addToPrecision = false

# Strengthen the abstraction formula during abstraction with invariants if
# some are generated. Invariants do not need to be generated with the
# PredicateCPA they can also be given from outside.
cpa.predicate.invariants.appendToAbstractionFormula = false

# Strengthen the pathformula during abstraction with invariants if some are
# generated. Invariants do not need to be generated with the PredicateCPA
# they can also be given from outside.
cpa.predicate.invariants.appendToPathFormula = false

# Should the automata used for invariant generation be dumped to files?
cpa.predicate.invariants.dumpInvariantGenerationAutomata = false

# Where to dump the automata that are used to narrow the analysis used for
# invariant generation.
cpa.predicate.invariants.dumpInvariantGenerationAutomataFile = "invgen.%d.spc"

# export final loop invariants
cpa.predicate.invariants.export = true

# export invariants as precision file?
cpa.predicate.invariants.exportAsPrecision = true

# file for exporting final loop invariants
cpa.predicate.invariants.file = "invariants.txt"

# Which strategy should be used for generating invariants, a comma separated
# list can be specified. Usually later specified strategies serve as fallback
# for earlier ones. (default is no invariant generation at all)
cpa.predicate.invariants.generationStrategy = new ArrayList<>()

# How often should generating invariants from sliced prefixes with
# k-induction be tried?
cpa.predicate.invariants.kInductionTries = 3

# file for precision that consists of invariants.
cpa.predicate.invariants.precisionFile = "invariantPrecs.txt"

# Timelimit for invariant generation which may be used during refinement.
# (Use seconds or specify a unit; 0 for infinite)
cpa.predicate.invariants.timeForInvariantGeneration = 10s

# Should the strategies be used all-together or only as fallback. If all
# together, the computation is done until the timeout is hit and the results
# up to this point are taken.
cpa.predicate.invariants.useAllStrategies = false

# Provide invariants generated with other analyses via the
# PredicateCPAInvariantsManager.
cpa.predicate.invariants.useGlobalInvariants = true

# Invariants that are not strong enough to refute the counterexample can be
# ignored with this option. (Weak invariants will lead to repeated
# counterexamples, thus taking time which could be used for the rest of the
# analysis, however, the found invariants may also be better for loops as
# interpolation.)
cpa.predicate.invariants.useStrongInvariantsOnly = true

# Max. number of edge of the abstraction tree to prescan for reuse
cpa.predicate.maxAbstractionReusePrescan = 1

# The maximum length for arrays (elements beyond this will be ignored). Use
# -1 to disable the limit.
cpa.predicate.maxArrayLength = 20

# Maximum size of allocations for which all structure fields are regarded
# always essential, regardless of whether they were ever really used in code.
cpa.predicate.maxPreFilledAllocationSize = 0

# Set of functions that non-deterministically provide new memory on the heap,
# i.e. they can return either a valid pointer or zero.
cpa.predicate.memoryAllocationFunctions = {
      "malloc", "__kmalloc", "kmalloc"
      }

# Memory allocation functions of which all parameters but the first should be
# ignored.
cpa.predicate.memoryAllocationFunctionsWithSuperfluousParameters = {
      "__kmalloc", "kmalloc", "kzalloc"}

# Set of functions that non-deterministically provide new zeroed memory on
# the heap, i.e. they can return either a valid pointer or zero.
cpa.predicate.memoryAllocationFunctionsWithZeroing = {"kzalloc", "calloc"}

# Setting this to true makes memoryAllocationFunctions always return a valid
# pointer.
cpa.predicate.memoryAllocationsAlwaysSucceed = false

# Function that is used to free allocated memory.
cpa.predicate.memoryFreeFunctionName = "free"

# which merge operator to use for predicate cpa (usually ABE should be used)
cpa.predicate.merge = "ABE"
  allowed values: [SEP, ABE]

# Set of functions that should be considered as giving a non-deterministic
# return value. If you specify this option, the default values are not added
# automatically to the list, so you need to specify them explicitly if you
# need them. Mentioning a function in this list has only an effect, if it is
# an 'external function', i.e., no source is given in the code for this
# function.
cpa.predicate.nondetFunctions = {
      "sscanf",
      "random"}

# Regexp pattern for functions that should be considered as giving a
# non-deterministic return value (c.f. cpa.predicate.nondedFunctions)
cpa.predicate.nondetFunctionsRegexp = "^(__VERIFIER_)?nondet_[a-zA-Z0-9_]*"

# Where to apply the found predicates to?
cpa.predicate.precision.sharing = LOCATION
  enum:     [GLOBAL, SCOPE, FUNCTION, LOCATION, LOCATION_INSTANCE]

# generate statistics about precisions (may be slow)
cpa.predicate.precisionStatistics = true

# Export the weakest precondition?
cpa.predicate.precondition.export = false

# File for exporting the weakest precondition.
cpa.predicate.precondition.file = "precondition.txt"

# export final predicate map
cpa.predicate.predmap.export = true

# file for exporting final predicate map
cpa.predicate.predmap.file = "predmap.txt"

# Format for exporting predicates from precisions.
cpa.predicate.predmap.predicateFormat = SMTLIB2
  enum:     [PLAIN, SMTLIB2]

# If an abstraction is computed during refinement, use only the interpolant
# as input, not the concrete block.
cpa.predicate.refinement.abstractInterpolantOnly = false

# use only the atoms from the interpolantsas predicates, and not the whole
# interpolant
cpa.predicate.refinement.atomicInterpolants = true

# use only atoms from generated invariantsas predicates, and not the whole
# invariant
cpa.predicate.refinement.atomicInvariants = false

# Direction for doing counterexample analysis: from start of trace, from end
# of trace, or alternatingly from start and end of the trace towards the
# middle
cpa.predicate.refinement.cexTraceCheckDirection = FORWARDS
  enum:     [FORWARDS, BACKWARDS, ZIGZAG, LOOP_FREE_FIRST, RANDOM, LOWEST_AVG_SCORE,
             HIGHEST_AVG_SCORE, LOOP_FREE_FIRST_BACKWARDS]

# Actually compute an abstraction, otherwise just convert the interpolants to
# BDDs as they are.
cpa.predicate.refinement.doAbstractionComputation = false

# dump all interpolation problems
cpa.predicate.refinement.dumpInterpolationProblems = false

# After each refinement, dump the newly found predicates.
cpa.predicate.refinement.dumpPredicates = false

# File name for the predicates dumped after refinements.
cpa.predicate.refinement.dumpPredicatesFile = "refinement%04d-predicates.prec"

# apply deletion-filter to the abstract counterexample, to get a minimal set
# of blocks, before applying interpolation-based refinement
cpa.predicate.refinement.getUsefulBlocks = false

# Do a complete restart (clearing the reached set) after the refinement
cpa.predicate.refinement.global.restartAfterRefinement = false

# Instead of updating precision and arg we say that the refinement was not
# successful after N times of refining. A real error state is not necessary
# to be found. Use 0 for unlimited refinements (default).
cpa.predicate.refinement.global.stopAfterNRefinements = 0

# use incremental search in counterexample analysis, to find the minimal
# infeasible prefix
cpa.predicate.refinement.incrementalCexTraceCheck = false

# Max. number of prefixes to extract
cpa.predicate.refinement.maxPrefixCount = 64

# Max. length of feasible prefixes to extract from if at least one prefix was
# already extracted
cpa.predicate.refinement.maxPrefixLength = 1024

# skip refinement if input formula is larger than this amount of bytes
# (ignored if 0)
cpa.predicate.refinement.maxRefinementSize = 0

# use heuristic to extract predicates from the CFA statically on first
# refinement
cpa.predicate.refinement.performInitialStaticRefinement = false

# Which predicates should be used as basis for a new precision.ALL: During
# refinement, keep predicates from all removed parts of the ARG.CUTPOINT:
# Only predicates from the cut-point's precision are kept.TARGET: Only
# predicates from the target state's precision are kept.
cpa.predicate.refinement.predicateBasisStrategy = TARGET
  enum:     [ALL, TARGET, CUTPOINT]

# which sliced prefix should be used for interpolation
cpa.predicate.refinement.prefixPreference = PrefixSelector.NO_SELECTION

# Do a complete restart (clearing the reached set) after N refinements. 0 to
# disable, 1 for always.
cpa.predicate.refinement.restartAfterRefinements = 0

# Use a single SMT solver environment for several interpolation queries
cpa.predicate.refinement.reuseInterpolationEnvironment = false

# During refinement, add all new predicates to the precisions of all abstract
# states in the reached set.
cpa.predicate.refinement.sharePredicates = false

# slice block formulas, experimental feature!
cpa.predicate.refinement.sliceBlockFormulas = false

# split each arithmetic equality into two inequalities when extracting
# predicates from interpolants
cpa.predicate.refinement.splitItpAtoms = false

# Strategy how to interact with the intepolating prover. The analysis must
# support the strategy, otherwise the result will be useless!
# - SEQ_CPACHECKER: We simply return each interpolant for i={0..n-1} for the
# partitions A=[0 .. i] and B=[i+1 .. n]. The result is similar to
# INDUCTIVE_SEQ, but we do not guarantee the 'inductiveness', i.e. the solver
# has to generate nice interpolants itself. Supported by all solvers!
# - INDUCTIVE_SEQ: Generate an inductive sequence of interpolants the
# partitions [1,...n]. 
# - TREE: use the tree-interpolation-feature of a solver to get interpolants
# - TREE_WELLSCOPED: We return each interpolant for i={0..n-1} for the
# partitions A=[lastFunctionEntryIndex .. i] and B=[0 ..
# lastFunctionEntryIndex-1 , i+1 .. n]. Based on a tree-like scheme.
# - TREE_NESTED: use callstack and previous interpolants for next
# interpolants (see 'Nested Interpolants'),
# - TREE_CPACHECKER: similar to TREE_NESTED, but the algorithm is taken from
# 'Tree Interpolation in Vampire'.
cpa.predicate.refinement.strategy = SEQ_CPACHECKER
  enum:     [SEQ, SEQ_CPACHECKER, TREE, TREE_WELLSCOPED, TREE_NESTED, TREE_CPACHECKER]

# time limit for refinement (use milliseconds or specify a unit; 0 for
# infinite)
cpa.predicate.refinement.timelimit = 0ms

# Use BDDs to simplify interpolants (removing irrelevant predicates)
cpa.predicate.refinement.useBddInterpolantSimplification = false

# Should the path invariants be created and used (potentially additionally to
# the other invariants)
cpa.predicate.refinement.usePathInvariants = false

# verify if the interpolants fulfill the interpolant properties
cpa.predicate.refinement.verifyInterpolants = false

# enable export of all relations that were collected to synthecise the
# abstract precision?
cpa.predicate.relations.export = false

# file that consists all relations that were collected to synthecise the
# abstract precision
cpa.predicate.relations.file = "relations.txt"

# Enable the option to allow detecting the allocation type by type of the LHS
# of the assignment, e.g. char *arr = malloc(size) is detected as char[size]
cpa.predicate.revealAllocationTypeFromLhs = true

# maximum blocksize before a satisfiability check is done
# (non-negative number, 0 means never, if positive should be smaller than
# blocksize)
cpa.predicate.satCheck = 0

# Enables sat checks at abstraction location.
# Infeasible paths are already excluded by transfer relation and not later by
# precision adjustment. This property is required in proof checking.
cpa.predicate.satCheckAtAbstraction = false

# Call 'simplify' on generated formulas.
cpa.predicate.simplifyGeneratedPathFormulas = false

# C99 only defines the overflow of unsigned integer type.
cpa.predicate.solver.ufCheckingProver.isSignedOverflowSafe = true

# How often should we try to get a better evaluation?
cpa.predicate.solver.ufCheckingProver.maxIterationNum = 5

# which stop operator to use for predicate cpa (usually SEP should be used in
# analysis)
cpa.predicate.stop = "SEP"
  allowed values: [SEP, SEPPCC]

# Use formula reporting states for strengthening.
cpa.predicate.strengthenWithFormulaReportingStates = false

# try to reuse old abstractions from file during strengthening
cpa.predicate.strengthenWithReusedAbstractions = false

# file that consists of old abstractions, to be used during strengthening
cpa.predicate.strengthenWithReusedAbstractionsFile = "abstractions.txt"

# The function used to model successful heap object allocation. This is only
# used, when pointer analysis with UFs is enabled.
cpa.predicate.successfulAllocFunctionName = "__VERIFIER_successful_alloc"

# The function used to model successful heap object allocation with zeroing.
# This is only used, when pointer analysis with UFs is enabled.
cpa.predicate.successfulZallocFunctionName = "__VERIFIER_successful_zalloc"

# whether to include the symbolic path formula in the coverage checks or do
# only the fast abstract checks
cpa.predicate.symbolicCoverageCheck = false

# check satisfiability when a target state has been found
cpa.predicate.targetStateSatCheck = false

# Whether to track values stored in variables of function-pointer type.
cpa.predicate.trackFunctionPointers = true

# Use the theory of arrays for heap-memory encoding. This requires an SMT
# solver that is capable of the theory of arrays.
cpa.predicate.useArraysForHeap = false

# try to add some useful static-learning-like axioms for bitwise operations
# (which are encoded as UFs): essentially, we simply collect all the numbers
# used in bitwise operations, and add axioms like (0 & n = 0)
cpa.predicate.useBitwiseAxioms = false

# Use regions for pointer analysis. So called Burstall&Bornat (BnB) memory
# regions will be used for pointer analysis. BnB regions are based not only
# on type, but also on structure field names. If the field is not accessed by
# an address then it is placed into a separate region.
cpa.predicate.useMemoryRegions = false

# add special information to formulas about non-deterministic functions
cpa.predicate.useNondetFlags = false

# Insert tmp-variables for parameters at function-entries. The variables are
# similar to return-variables at function-exit.
cpa.predicate.useParameterVariables = false

# Insert tmp-parameters for global variables at function-entries. The global
# variables are also encoded with return-variables at function-exit.
cpa.predicate.useParameterVariablesForGlobals = false

# Use quantifiers when encoding heap accesses. This requires an SMT solver
# that is capable of quantifiers (e.g. Z3 or PRINCESS).
cpa.predicate.useQuantifiersOnArrays = false

# Enable fallback to UFs if a solver does not support non-linear arithmetics.
# This option only effects MULT, MOD and DIV.
cpa.predicate.useUFsForNonLinearArithmetic = true

# Do not follow states which can not syntactically lead to a target location
cpa.property_reachability.noFollowBackwardsUnreachable = true

# Qualified name for class which checks that the computed abstraction adheres
# to the desired property.
cpa.propertychecker.className = org.sosy_lab.cpachecker.pcc.propertychecker.DefaultPropertyChecker.class

# List of parameters for constructor of propertychecker.className. Parameter
# values are specified in the order the parameters are defined in the
# respective constructor. Every parameter value is finished with ",". The
# empty string represents an empty parameter list.
cpa.propertychecker.parameters = ""

# which merge operator to use for ReachingDefCPA
cpa.reachdef.merge = "JOIN"
  allowed values: [SEP, JOIN, IGNORECALLSTACK]

# which stop operator to use for ReachingDefCPA
cpa.reachdef.stop = "SEP"
  allowed values: [SEP, JOIN, IGNORECALLSTACK]

# Do not report 'False' result, return UNKNOWN instead.  Useful for
# incomplete analysis with no counterexample checking.
cpa.reportFalseAsUnknown = false

# which merge operator to use for SignCPA
cpa.sign.merge = "JOIN"
  allowed values: [SEP, JOIN]

# which stop operator to use for SignCPA
cpa.sign.stop = "SEP"
  allowed values: [SEP, JOIN]

# Apply AND- LBE transformation to loop transition relation.
cpa.slicing.applyLBETransformation = true

# Check target states reachability
cpa.slicing.checkTargetStates = true

# Filter lemmas by liveness
cpa.slicing.filterByLiveness = true

# Instead of considering the entire SCC, ignore the function nested in the
# loop. UNSOUND!
cpa.slicing.ignoreFunctionCallsInLoop = false

# Depth limit for the 'LEAST_REMOVALS' strategy.
cpa.slicing.leastRemovalsDepthLimit = 2

# Pre-run syntactic weakening
cpa.slicing.preRunSyntacticWeakening = true

# Strategy for abstracting children during CEX weakening
cpa.slicing.removalSelectionStrategy = ALL
  enum:     [ALL, FIRST, RANDOM, LEAST_REMOVALS]

# Time for loop generation before aborting.
# (Use seconds or specify a unit; 0 for infinite)
cpa.slicing.timeForLoopGeneration = 0s

# Inductive weakening strategy
cpa.slicing.weakeningStrategy = CEX
  enum:     [SYNTACTIC, DESTRUCTIVE, CEX]

# Array allocation functions
cpa.smg.arrayAllocationFunctions = {
      "calloc"}

# with this option enabled, a check for unreachable memory occurs whenever a
# function returns, and not only at the end of the main function
cpa.smg.checkForMemLeaksAtEveryFrameDrop = true

# Deallocation functions
cpa.smg.deallocationFunctions = {
      "free"}

# with this option enabled, heap abstraction will be enabled.
cpa.smg.enableHeapAbstraction = false

# If this Option is enabled, failure of mallocis simulated
cpa.smg.enableMallocFail = true

# Filename format for SMG graph dumps
cpa.smg.exportSMG.file = "smg/smg-%s.dot"

# Describes when SMG graphs should be dumped.
cpa.smg.exportSMGwhen = NEVER
  enum:     [NEVER, LEAF, INTERESTING, EVERY]

# Functions which indicate on external allocated memory
cpa.smg.externalAllocationFunction = {
      "ext_allocation"}

# Default size of externally allocated memory
cpa.smg.externalAllocationSize = Integer.MAX_VALUE

# Size of memory that cannot be calculated will be guessed.
cpa.smg.guessSizeOfUnknownMemorySize = false

# with this option enabled, memory that is not freed before the end of main
# is reported as memleak even if it is reachable from local variables in main
cpa.smg.handleNonFreedMemoryInMainAsMemLeak = false

# Sets how unknown functions are handled.
cpa.smg.handleUnknownFunctions = STRICT
  enum:     [STRICT, ASSUME_SAFE]

# Memory allocation functions
cpa.smg.memoryAllocationFunctions = {
      "malloc"}

# Size parameter of memory allocation functions
cpa.smg.memoryAllocationFunctionsSizeParameter = 0

# Position of element size parameter for array allocation functions
cpa.smg.memoryArrayAllocationFunctionsElemSizeParameter = 1

# Position of number of element parameter for array allocation functions
cpa.smg.memoryArrayAllocationFunctionsNumParameter = 0

# Determines if memory errors are target states
cpa.smg.memoryErrors = true

# export interpolant smgs for every path interpolation to this path template
cpa.smg.refinement.exportInterpolantSMGs = "smg/interpolation-%d/%s"

# when to export the interpolation tree
# NEVER:   never export the interpolation tree
# FINAL:   export the interpolation tree once after each refinement
# ALWAYS:  export the interpolation tree once after each interpolation, i.e.
# multiple times per refinement
cpa.smg.refinement.exportInterpolationTree = "NEVER"
  allowed values: [NEVER, FINAL, ALWAYS]

# export interpolant smgs for every path interpolation to this path template
cpa.smg.refinement.exportRefinmentSMGs = "smg/refinment-%d/smg-%s"

# export interpolation trees to this file template
cpa.smg.refinement.interpolationTreeExportFile = "interpolationTree.%d-%d.dot"

# Sets the level of runtime checking: NONE, HALF, FULL
cpa.smg.runtimeCheck = NONE
  enum:     [FORCED, NONE, HALF, FULL]

# which stop operator to use for the SMGCPA
cpa.smg.stop = "SEP"
  allowed values: [SEP, NEVER, END_BLOCK]

# Enable track predicates on SMG state
cpa.smg.trackPredicates = false

# Emit messages when we encounter non-target undefined behavior
cpa.smg.unknownOnUndefined = true

# Allow SMG to check predicates
cpa.smg.verifyPredicates = false

# Allocation functions which set memory to zero
cpa.smg.zeroingMemoryAllocation = {
      "calloc"}

# set this to true when you only want to do a code analysis. If StatisticsCPA
# is combined with other CPAs to do queries use false.
cpa.statistics.analysis = true

# which merge operator to use for StatisticsCPA? Ignored when analysis is set
# to true
cpa.statistics.mergeSep = "sep"
  allowed values: [sep, join]

# count the number of traversed arithmetic operations.
cpa.statistics.metric.arithmeticOperationCount = true

# count the number of traversed variable definitions with array type.
cpa.statistics.metric.arrayVariablesCount = true

# count the number of traversed assume statements.
cpa.statistics.metric.assumeCount = true

# count the number of traversed bitwise operations.
cpa.statistics.metric.bitwiseOperationCount = true

# count the number of traversed edges with more then one outgoing edge.
cpa.statistics.metric.branchCount = true

# count the number of traversed dereference operations.
cpa.statistics.metric.dereferenceCount = true

# count the number of traversed variable definitions with floating type
# (float or double).
cpa.statistics.metric.floatVariablesCount = true

# count the number of traversed function calls.
cpa.statistics.metric.functionCallCount = true

# count the number of traversed function definitions.
cpa.statistics.metric.functionDefCount = true

# count the number of traversed global variable definitions.
cpa.statistics.metric.globalVariablesCount = true

# count the number of traversed gotos.
cpa.statistics.metric.gotoCount = true

# count the number of traversed variable definitions with integer type.
cpa.statistics.metric.integerVariablesCount = true

# count the number of traversed jumps.
cpa.statistics.metric.jumpCount = true

# count the number of traversed local variable definitions.
cpa.statistics.metric.localVariablesCount = true

# count the number of traversed loops.
cpa.statistics.metric.loopCount = true

# count the number of traversed nodes.
cpa.statistics.metric.nodeCount = true

# count the number of traversed variable definitions with pointer type.
cpa.statistics.metric.pointerVariablesCount = true

# count the number of traversed variable definitions with a complex structure
# type.
cpa.statistics.metric.structVariablesCount = true

# target file to hold the statistics
cpa.statistics.statisticsCPAFile = no default value

# Which refinement algorithm to use? (give class name, required for
# termination algorithm with CEGAR) If the package name starts with
# 'org.sosy_lab.cpachecker.', this prefix can be omitted.
cpa.termination.refiner = no default value

# allow assignments of a new thread to the same left-hand-side as an existing
# thread.
cpa.threading.allowMultipleLHS = false

# the maximal number of parallel threads, -1 for infinite. When combined with
# 'useClonedFunctions=true', we need at least N cloned functions. The option
# 'cfa.cfaCloner.numberOfCopies' should be set to N.
cpa.threading.maxNumberOfThreads = 5

# atomic locks are used to simulate atomic statements, as described in the
# rules of SV-Comp.
cpa.threading.useAtomicLocks = true

# do not use the original functions from the CFA, but cloned ones. See
# cfa.postprocessing.CFACloner for detail.
cpa.threading.useClonedFunctions = true

# local access locks are used to avoid expensive interleaving, if a thread
# only reads and writes its own variables.
cpa.threading.useLocalAccessLocks = true

# which merge operator to use for UninitializedVariablesCPA?
cpa.uninitvars.merge = "sep"
  allowed values: [sep, join]

# print warnings during analysis when uninitialized variables are used
cpa.uninitvars.printWarnings = "true"

# which stop operator to use for UninitializedVariablesCPA?
cpa.uninitvars.stop = "sep"
  allowed values: [sep, join]

# which merge operator to use for ValidVarsCPA
cpa.validVars.merge = "JOIN"
  allowed values: [SEP, JOIN]

# restrict abstraction computations to branching points
cpa.value.abstraction.alwaysAtBranch = false

# restrict abstraction computations to function calls/returns
cpa.value.abstraction.alwaysAtFunction = false

# restrict abstraction computations to join points
cpa.value.abstraction.alwaysAtJoin = false

# restrict abstraction computations to loop heads
cpa.value.abstraction.alwaysAtLoop = false

# threshold for level of determinism, in percent, up-to which abstraction
# computations are performed (and iteration threshold was reached)
cpa.value.abstraction.determinismThreshold = 85

# toggle liveness abstraction
cpa.value.abstraction.doLivenessAbstraction = false

# skip abstraction computations until the given number of iterations are
# reached, after that decision is based on then current level of determinism,
# setting the option to -1 always performs abstraction computations
cpa.value.abstraction.iterationThreshold = -1

# restrict liveness abstractions to nodes with more than one entering and/or
# leaving edge
cpa.value.abstraction.onlyAtNonLinearCFA = false

# Whether to replace symbolic values with a concrete value when only one
# value is possible for an assumption to be true (e.g. for (x == 1) set x to
# 1, even if x is a symbolic expression).
cpa.value.assignSymbolicAssumptionVars = false

# if there is an assumption like (x!=0), this option sets unknown
# (uninitialized) variables to 1L, when the true-branch is handled.
cpa.value.initAssumptionVars = false

# get an initial precision from file
cpa.value.initialPrecisionFile = no default value

# apply optimizations based on equality of input interpolant and candidate
# interpolant
cpa.value.interpolation.applyItpEqualityOptimization = true

# apply optimizations based on CFA edges with only variable-renaming
# semantics
cpa.value.interpolation.applyRenamingOptimization = true

# apply optimizations based on infeasibility of suffix
cpa.value.interpolation.applyUnsatSuffixOptimization = true

# whether or not to manage the callstack, which is needed for BAM
cpa.value.interpolation.manageCallstack = true

# which merge operator to use for ValueAnalysisCPA
cpa.value.merge = "SEP"
  allowed values: [SEP, JOIN]

# Assume that variables used only in a boolean context are either zero or
# one.
cpa.value.optimizeBooleanVariables = true

# target file to hold the exported precision
cpa.value.precisionFile = no default value

# whether or not to add assumptions to counterexamples, e.g., for supporting
# counterexample checks
cpa.value.refinement.addAssumptionsToCex = true

# whether or not to use heuristic to avoid similar, repeated refinements
cpa.value.refinement.avoidSimilarRepeatedRefinement = false

# whether or not to do lazy-abstraction
cpa.value.refinement.doLazyAbstraction = true

# when to export the interpolation tree
# NEVER:   never export the interpolation tree
# FINAL:   export the interpolation tree once after each refinement
# ALWAYS:  export the interpolation tree once after each interpolation, i.e.
# multiple times per refinement
cpa.value.refinement.exportInterpolationTree = "NEVER"
  allowed values: [NEVER, FINAL, ALWAYS]

# export interpolation trees to this file template
cpa.value.refinement.interpolationTreeExportFile = "interpolationTree.%d-%d.dot"

# heuristic to sort targets based on the quality of interpolants derivable
# from them
cpa.value.refinement.itpSortedTargets = false

# whether or not to perform path slicing before interpolation
cpa.value.refinement.pathSlicing = true

# whether to perform (more precise) edge-based interpolation or (more
# efficient) path-based interpolation
cpa.value.refinement.performEdgeBasedInterpolation = true

# which prefix of an actual counterexample trace should be used for
# interpolation
cpa.value.refinement.prefixPreference = [PrefixPreference.DOMAIN_MIN, PrefixPreference.LENGTH_MIN]

# whether or not to do lazy-abstraction
cpa.value.refinement.restart = PIVOT
  enum:     [ROOT, PIVOT, COMMON]

# instead of reporting a repeated counter-example, search and refine another
# error-path for the same target-state.
cpa.value.refinement.searchForFurtherErrorPaths = false

# store all refined paths
cpa.value.refinement.storeAllRefinedPaths = false

# whether to use the top-down interpolation strategy or the bottom-up
# interpolation strategy
cpa.value.refinement.useTopDownInterpolationStrategy = true

# which stop operator to use for ValueAnalysisCPA
cpa.value.stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

# Whether to adopt definite assignments computed by the ConstraintsCPA
cpa.value.symbolic.adoptDefinites = true

# Default size of arrays whose length can't be determined.
cpa.value.symbolic.defaultArraySize = 20

# If this option is set to true, an own symbolic identifier is assigned to
# each array slot when handling non-deterministic arrays of fixed length. If
# the length of the array can't be determined, it won't be handled in either
# cases.
cpa.value.symbolic.handleArrays = false

# Whether to handle non-deterministic pointers in symbolic value analysis.
cpa.value.symbolic.handlePointers = true

# If this option is set to true, an own symbolic identifier is assigned to
# each struct member when handling non-deterministic structs.
cpa.value.symbolic.handleStructs = true

# Whether to try to not use any constraints in refinement
cpa.value.symbolic.refinement.avoidConstraints = true

# The refinement strategy to use
cpa.value.symbolic.refinement.strategy = CONSTRAINTS_FIRST
  enum:     [CONSTRAINTS_FIRST, VALUES_FIRST, VALUES_ONLY]

# Whether to simplify symbolic expressions, if possible.
cpa.value.symbolic.simplifySymbolics = true

# Use symbolic values. This allows tracking of non-deterministic values.
# Symbolic values should always be used in conjunction with ConstraintsCPA.
# Otherwise, symbolic values will be created, but not evaluated.
cpa.value.symbolic.useSymbolicValues = false

# Track Java array values in explicit value analysis. This may be costly if
# the verified program uses big or lots of arrays. Arrays in C programs will
# always be tracked, even if this value is false.
cpa.value.trackJavaArrayValues = true

# Specify simple custom instruction by specifying the binary operator op. All
# simple cis are of the form r = x op y. Leave empty (default) if you specify
# a more complex custom instruction within code.
custominstructions.binaryOperatorForSimpleCustomInstruction = ""
  allowed values: [MULTIPLY, DIVIDE, MODULO, PLUS, MINUS, SHIFT_LEFT, SHIFT_RIGHT, LESS_THAN, GREATER_THAN, LESS_EQUAL, GREATER_EQUAL, BINARY_AND, BINARY_XOR, BINARY_OR, EQUALS, NOT_EQUALS, ]

# Signature for custom instruction, describes names and order of input and
# output variables of a custom instruction
custominstructions.ciSignature = "ci_spec.txt"

# File to be parsed
custominstructions.definitionFile = no default value

# Where to dump the requirements on custom instruction extracted from
# analysis
custominstructions.dumpCIRequirements = "ci%d.smt"

# Try to remove informations from requirements which is irrelevant for custom
# instruction behavior
custominstructions.enableRequirementSlicing = false

# Qualified name of class for abstract state which provides custom
# instruction requirements.
custominstructions.requirementsStateClassName = no default value

# Option to change the behaviour of the loop detection for generating the
# Counterexample-C-Code that will probably be used to generate invariants.
# Note that last loop means the first loop encountered when backwards
# traversing the given ARGPath, thus, the last loop may contain other loops,
# which are in turn also counted to the last loop.
cwriter.withLoops.loopDetectionStrategy = ALL_LOOPS
  enum:     [ALL_LOOPS, ONLY_LAST_LOOP]

# Enable to use lazy refinement in current analysis instead of restarting
# from root after each refinement.
enabledanalysis.allowLazyRefinement = false

# Which CPA is used as enabler in the current analysis.
enabledanalysis.enablerCPA = PREDICATE
  enum:     [APRON, INTERVAL, OCTAGON, PREDICATE, VALUE]

# enable the Forced Covering optimization
impact.useForcedCovering = true

# Configuration file for the K-Induction algorithm for checking candidates on
# invariance.
invariantChecker.kInductionConfig = "config/bmc-invgen.properties"

# configuration file for invariant generation
invariantGeneration.config = no default value

# Check candidate invariants in a separate thread asynchronously.
invariantGeneration.kInduction.async = true

# Guess some candidates for the k-induction invariant generator from the CFA.
invariantGeneration.kInduction.guessCandidatesFromCFA = true

# Provides additional candidate invariants to the k-induction invariant
# generator.
invariantGeneration.kInduction.invariantsAutomatonFile = no default value

# For correctness-witness validation: Shut down if a candidate invariant is
# found to be incorrect.
invariantGeneration.kInduction.terminateOnCounterexample = false

# Specify the class code path to search for java class or interface
# definitions
java.classpath = ""

# use the following encoding for java files
java.encoding = StandardCharsets.UTF_8

# export TypeHierarchy as .dot file
java.exportTypeHierarchy = true

# Specify the source code path to search for java class or interface
# definitions
java.sourcepath = ""

# export TypeHierarchy as .dot file
java.typeHierarchyFile = "typeHierarchy.dot"

# Specifies the java version of source code accepted
java.version = JavaCore.VERSION_1_7

# C or Java?
language = C
  enum:     [C, JAVA]

# Limit for cpu time used by CPAchecker (use seconds or specify a unit; -1
# for infinite)
limits.time.cpu = -1ns

# Limit for thread cpu time used by CPAchecker. This option will in general
# not work when multi-threading is used in more than one place, use only with
# great caution! (use seconds or specify a unit; -1 for infinite)
limits.time.cpu.thread = -1ns

# Limit for wall time used by CPAchecker (use seconds or specify a unit; -1
# for infinite)
limits.time.wall = -1ns

# By changing this option one can adjust the way how live variables are
# created. Function-wise means that each function is handled separately,
# global means that the whole cfa is used for the computation.
liveVar.evaluationStrategy = FUNCTION_WISE
  enum:     [FUNCTION_WISE, GLOBAL]

# Overall timelimit for collecting the liveness information.(use seconds or
# specify a unit; 0 for infinite)
liveVar.overallLivenessCheckTime = 0ns

# Timelimit for collecting the liveness information with one approach, (p.e.
# if global analysis is selected and fails in the specified timelimit the
# function wise approach will have the same time-limit afterwards to compute
# the live variables).(use seconds or specify a unit; 0 for infinite)
liveVar.partwiseLivenessCheckTime = 20s

# Write the tokenized version of the input program to this file.
locmapper.dumpTokenizedProgramToFile = no default value

# all used options are printed
log.usedOptions.export = false

# Whether to check for memory safety properties (this can be specified by
# passing an appropriate .prp file to the -spec parameter).
memorysafety.check = false

# When checking for memory safety properties, use this configuration file
# instead of the current one.
memorysafety.config = no default value

# which merge operator to use for LiveVariablesCPA
merge = "JOIN"
  allowed values: [SEP, JOIN]

# Whether to check for the overflow property (this can be specified by
# passing an appropriate .prp file to the -spec parameter).
overflow.check = false

# When checking for the overflow property, use this configuration file
# instead of the current one.
overflow.config = no default value

# Track overflow for signed integers.
overflow.trackSignedIntegers = true

# Only check live variables for overflow, as compiler can remove dead
# variables.
overflow.useLiveness = true

# List of files with configurations to use. Files can be suffixed with
# ::supply-reached this signalizes that the (finished) reached set of an
# analysis can be used in other analyses (e.g. for invariants computation).
# If you use the suffix ::supply-reached-refinable instead this means that
# the reached set supplier is additionally continously refined (so one of the
# analysis has to be instanceof ReachedSetAdjustingCPA) to make this work
# properly.
parallelAlgorithm.configFiles = no default value

# C dialect for parser
parser.dialect = GNUC
  enum:     [C99, GNUC]

# The command line for calling the preprocessor. May contain binary name and
# arguments, but won't be expanded by a shell. The source file name will be
# appended to this string. The preprocessor needs to print the output to
# stdout.
parser.preprocessor = "cpp"

# Directory where to dump the results of the preprocessor.
parser.preprocessor.dumpDirectory = "preprocessed"

# Whether to dump the results of the preprocessor to disk for debugging.
parser.preprocessor.dumpResults = false

# For C files, read #line preprocessor directives and use their information
# for outputting line numbers. (Always enabled when pre-processing is used.)
parser.readLineDirectives = false

# Preprocess the given C files before parsing: Put every single token onto a
# new line. Then the line number corresponds to the token number.
parser.transformTokensToLines = false

# For C files, run the preprocessor on them before parsing. Note that all
# file numbers printed by CPAchecker will refer to the pre-processed file,
# not the original input file.
parser.usePreprocessor = false

# Enable if used property checker implements satisfiesProperty(AbstractState)
# and checked property is violated for a set iff an element in this set
# exists for which violates the property
pcc.arg.checkPropertyPerElement = false

# Enable to store ARG states instead of abstract states wrapped by ARG state
pcc.backwardtargets.certificateStatesAsARGStates = false

# List of files with configurations to use. 
pcc.cmc.configFiles = no default value

# write collected assumptions to file
pcc.cmc.file = "AssumptionAutomaton.txt"

# collects information about value analysis states in proof
pcc.collectValueAnalysisStateInfo = false

# The number of cores used exclusively for proof reading. Must be less than
# pcc.useCores and may not be negative. Value 0 means that the cores used for
# reading and checking are shared
pcc.interleaved.useReadCores = 0

# enables parallel checking of partial certificate
pcc.parallel.io.enableParallelCheck = false

# Selects the strategy used for partial certificate construction
pcc.partial.certificateType = HEURISTIC
  enum:     [ALL, HEURISTIC, ARG, MONOTONESTOPARG]

# If enabled, distributes checking of partial elements depending on actual
# checking costs, else uses the number of elements
pcc.partial.enableLoadDistribution = false

# Enables proper PCC but may not work correctly for heuristics. Stops adding
# newly computed elements to reached set if size saved in proof is reached.
# If another element must be added, stops certificate checking and returns
# false.
pcc.partial.stopAddingAtReachedSetSize = false

# [Best-first] Balance criterion for pairwise optimization of partitions
pcc.partitioning.bestfirst.balancePrecision = 1.0d

# Evaluation function to determine exploration order of best-first-search
pcc.partitioning.bestfirst.chosenFunction = BEST_IMPROVEMENT_FIRST
  enum:     [BREADTH_FIRST, DEPTH_FIRST, BEST_IMPROVEMENT_FIRST]

# Balance criterion for pairwise optimization of partitions
pcc.partitioning.fm.balanceCriterion = 1.5d

# Heuristic for computing an initial partitioning of proof
pcc.partitioning.fm.initialPartitioningStrategy = RANDOM
  enum:     [RANDOM]

# [FM-k-way] Balance criterion for pairwise optimization of partitions
pcc.partitioning.kwayfm.balancePrecision = 1.3d

# [FM-k-way] Partitioning method to compute initial partitioning.
pcc.partitioning.kwayfm.globalHeuristic = BEST_IMPROVEMENT_FIRST
  enum:     [RANDOM, DFS, BFS, BEST_IMPROVEMENT_FIRST]

# [FM-k-way] Local optimization criterion to be minimized druing
# Fiduccia/Mattheyses refinment
pcc.partitioning.kwayfm.optimizationCriterion = NODECUT
  enum:     [EDGECUT, NODECUT]

# Specifies the maximum size of the partition. This size is used to compute
# the number of partitions if a proof (reached set) should be written.
# Default value 0 means always a single partition.
pcc.partitioning.maxNumElemsPerPartition = 0

# Partitioning method applied in multilevel heuristic to compute initial
# partitioning.
pcc.partitioning.multilevel.globalHeuristic = BEST_IMPROVEMENT_FIRST
  enum:     [RANDOM, DFS, BFS, BEST_IMPROVEMENT_FIRST]

# Matching method applied to coarsen graph down in multilevel heuristic.
pcc.partitioning.multilevel.matchingGenerator = HEAVY_EDGE
  enum:     [RANDOM, HEAVY_EDGE]

# Refinement method applied in multilevel heuristic's uncoarsening phase.
pcc.partitioning.multilevel.refinementHeuristic = FM_NODECUT
  enum:     [FM_NODECUT, FM_EDGECUT]

# Heuristic for computing partitioning of proof (partial reached set).
pcc.partitioning.partitioningStrategy = RANDOM
  enum:     [RANDOM, DFS, BFS, OPTIMAL, BEST_FIRST, FM, FM_K_WAY, MULTILEVEL]

# If enabled uses the number of nodes saved in certificate to compute
# partition number otherwise the number of states explored during analysis
pcc.partitioning.useGraphSizeToComputePartitionNumber = false

# file in which proof representation needed for proof checking is stored
pcc.proof = "arg.obj"

# file in which proof representation will be stored
pcc.proofFile = "arg.obj"

# Generate and dump a proof
pcc.proofgen.doPCC = false

# Configuration for proof checking if differs from analysis configuration
pcc.resultcheck.checkerConfig = no default value

# Enable to write proof and read it again for validation instead of using the
# in memory solution
pcc.resultcheck.writeProof = false

# Make proof more abstract, remove some of the information not needed to
# prove the property.
pcc.sliceProof = false

# Qualified name for class which implements certification strategy, hence
# proof writing, to be used.
pcc.strategy = no default value

# number of cpus/cores which should be used in parallel for proof checking
pcc.useCores = 1

# whether to track relevant variables only at the exact program location
# (sharing=location), or within their respective (function-/global-) scope
# (sharing=scoped).
precision.sharing = SCOPE
  enum:     [SCOPE, LOCATION]

# Allowed coefficients in a template.
precision.template.allowedCoefficients = {
      Rational.NEG_ONE, Rational.ONE
  }

# Generate difference constraints.This option is redundant for
# `maxExpressionSize` >= 2.
precision.template.generateDifferences = false

# Generate templates from assert statements
precision.template.generateFromAsserts = true

# Generate templates from all program statements
precision.template.generateFromStatements = false

# Force the inclusion of function parameters into the generated templates.
# Required for summaries computation.
precision.template.includeFunctionParameters = false

# Maximum size for the generated template
precision.template.maxExpressionSize = 1

# Perform refinement using enumerative template synthesis.
precision.template.performEnumerativeRefinement = true

# Do not generate templates with threshold larger than specified. Set to '-1'
# for no limit.
precision.template.templateConstantThreshold = 100

# Strategy for filtering variables out of templates using liveness
precision.template.varFiltering = ALL_LIVE
  enum:     [ALL_LIVE, ONE_LIVE, ALL]

# If this option is used, variables that are addressed may get tracked
# depending on the rest of the precision. When this option is disabled, a
# variable that is addressed is definitely not tracked.
precision.trackAddressedVariables = true

# If this option is used, booleans from the cfa are tracked.
precision.trackBooleanVariables = true

# If this option is used, variables that have type double or float are
# tracked.
precision.trackFloatVariables = true

# If this option is used, variables, that are only used in simple
# calculations (add, sub, lt, gt, eq) are tracked.
precision.trackIntAddVariables = true

# If this option is used, variables that are only compared for equality are
# tracked.
precision.trackIntEqualVariables = true

# If this option is used, all variables that are of a different
# classification than IntAdd, IntEq and Boolean get tracked by the precision.
precision.trackVariablesBesidesEqAddBool = true

# blacklist regex for variables that won't be tracked by the CPA using this
# precision
precision.variableBlacklist = ""

# whitelist regex for variables that will always be tracked by the CPA using
# this precision
precision.variableWhitelist = ""

# List of property files (INTERNAL USAGE ONLY - DO NOT USE)
properties = []

# Quantifier elimination strategy
rcnf.boundVarsHandling = QE_LIGHT_THEN_DROP
  enum:     [QE_LIGHT_THEN_DROP, QE, DROP]

# Expand equality atoms. E.g. 'x=a' gets expanded into 'x >= a AND x <= a'.
# Can lead to stronger weakenings.
rcnf.expandEquality = false

# Limit on the size of the resulting number of lemmas from the explicit
# expansion
rcnf.expansionResultSizeLimit = 100

# print reached set to graph file
reachedSet.dot = "reached.dot"

# print reached set to text file
reachedSet.export = false
reachedSet.file = "reached.txt"

# Generate HTML report with analysis result.
report.export = true

# File name for analysis report in case no counterexample was found.
report.file = "Report.html"

# combine (partial) ARGs obtained by restarts of the analysis after an
# unknown result with a different configuration
restartAlgorithm.combineARGsAfterRestart = false

# List of files with configurations to use. A filename can be suffixed with
# :if-interrupted, :if-failed, and :if-terminated which means that this
# configuration will only be used if the previous configuration ended with a
# matching condition. What also can be added is :use-reached then the reached
# set of the preceding analysis is taken and provided to the next analysis.
restartAlgorithm.configFiles = no default value

# print the statistics of each component of the restart algorithm directly
# after the components computation is finished
restartAlgorithm.printIntermediateStatistics = true

# List of files with configurations to use. 2 filenames expected.
restartAlgorithmWithARGReplay.configFiles = no default value

# Extract and cache unsat cores for satisfiability checking
solver.cacheUnsatCores = true

# improve sat-checks with additional constraints for UFs
solver.checkUFs = false

# Which solver to use specifically for interpolation (default is to use the
# main one).
solver.interpolationSolver = no default value
  enum:     [MATHSAT5, SMTINTERPOL, Z3, PRINCESS]

# Which SMT solver to use.
solver.solver = SMTINTERPOL
  enum:     [MATHSAT5, SMTINTERPOL, Z3, PRINCESS]

# File for exporting the path automaton in DOT format.
spec.automatonDumpFile = no default value

# Consider assumptions that are provided with the path automaton?
spec.considerAssumptions = true

# Match the branching information at a branching location.
spec.matchAssumeCase = true

# Match the character offset within the file.
spec.matchOffset = true

# Match the line numbers within the origin (mapping done by preprocessor line
# markers).
spec.matchOriginLine = true

# Match the source code provided with the witness.
spec.matchSourcecodeData = false

# Do not try to "catch up" with witness guards: If they do not match, go to
# the sink.
spec.strictMatching = false

# Legacy option for token-based matching with path automatons.
spec.transitionToStopForNegatedTokensetMatch = false

# comma-separated list of files with specifications that should be checked
# (see config/specification/ for examples)
specification = []

# export abstract states as formula, e.g. for re-using them as
# PredicatePrecision.
statesToFormulas.exportFile = no default value

# export formulas for all program locations or just the important
# locations,which include loop-heads, funtion-calls and function-exits.
statesToFormulas.exportOnlyImporantLocations = false

# instead of writing the exact state-representation as a single formula,
# write its atoms as a list of formulas. Therefore we ignore operators for
# conjunction and disjunction.
statesToFormulas.splitFormulas = LOCATION
  enum:     [LOCATION, STATE, ATOM]

# Add all assumtions from the control flow automaton to the precision.
staticRefiner.addAllControlFlowAssumes = false

# Add all assumtions along a error trace to the precision.
staticRefiner.addAllErrorTraceAssumes = false
staticRefiner.addAssumesByBoundedBackscan = true

# Apply mined predicates on the corresponding scope. false = add them to the
# global precision.
staticRefiner.applyScoped = true

# Dump CFA assume edges as SMTLIB2 formulas to a file.
staticRefiner.assumePredicatesFile = no default value

# split generated heuristic predicates into atoms
staticRefiner.atomicPredicates = true

# collect at most this number of assumes along a path, backwards from each
# target (= error) location
staticRefiner.maxBackscanPathAssumes = 1

# write some statistics to disk
statistics.export = true
statistics.file = "Statistics.txt"

# track memory usage of JVM during runtime
statistics.memory = true

# print statistics to console
statistics.print = false

# which stop operator to use for LiveVariablesCPA
stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

# Whether to check for the termination property (this can be specified by
# passing an appropriate .prp file to the -spec parameter).
termination.check = false

# When checking for the termination property, use this configuration file
# instead of the current one.
termination.config = no default value

# Number of generalized eigenvectors in the geometric nontermination
# argument.
termination.lassoAnalysis.eigenvectors = 3

# Shell command used to call the external SMT solver.
termination.lassoAnalysis.externalSolverCommand = "./lib/native/x86_64-linux/z3 -smt2 -in SMTLIB2_COMPLIANT=true "

# Analysis type used for synthesis of linear termination arguments.
termination.lassoAnalysis.linear.analysisType = LINEAR_WITH_GUESSES
  enum:     [DISABLED, LINEAR, LINEAR_WITH_GUESSES, NONLINEAR]

# If true, an external tool is used as SMT solver instead of SMTInterpol.
# This affects only synthesis of linear termination arguments.
termination.lassoAnalysis.linear.externalSolver = false

# Maximal number of functions used in a ranking function template.
termination.lassoAnalysis.maxTemplateFunctions = 3

# Number of non-strict supporting invariants for each Motzkin transformation
# during synthesis of termination arguments.
termination.lassoAnalysis.nonStrictInvariants = 3

# Analysis type used for synthesis of non-linear termination arguments.
termination.lassoAnalysis.nonlinear.analysisType = LINEAR_WITH_GUESSES
  enum:     [DISABLED, LINEAR, LINEAR_WITH_GUESSES, NONLINEAR]

# If true, an external tool is used as SMT solver instead of SMTInterpol.
# This affects only synthesis of non-linear termination arguments and
# non-termination arguments.
termination.lassoAnalysis.nonlinear.externalSolver = false

# Number of strict supporting invariants for each Motzkin transformation
# during synthesis of termination arguments.
termination.lassoAnalysis.strictInvariants = 2

# Simplifies loop and stem formulas.
termination.lassoBuilder.simplify = false

# maximal number of repeated ranking functions per loop before stopping
# analysis
termination.maxRepeatedRankingFunctionsPerLoop = 10

# Strategy used to prepare reched set and ARG for next iteration after
# successful refinement of the termination argument.
termination.resetReachedSetStrategy = REMOVE_LOOP
  enum:     [REMOVE_TARGET_STATE, REMOVE_LOOP, RESET]

# A human readable representation of the synthesized (non-)termination
# arguments is exported to this file.
termination.resultFile = "terminationAnalysisResult.txt"

# Use the counterexample model to provide test-vector values
testHarnessExport.useModel = true
